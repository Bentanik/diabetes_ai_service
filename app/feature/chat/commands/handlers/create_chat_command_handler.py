import os
import dotenv
import asyncio
from datetime import datetime
from typing import List, Optional, Dict
from bson import ObjectId
from tenacity import retry, stop_after_attempt, wait_exponential

from core.cqrs import CommandRegistry, CommandHandler, Mediator
from core.llm import QwenLLM
from core.result import Result
from shared.messages import ChatMessage, SettingMessage
from app.feature.train_ai import GetRetrievedContextQuery
from app.database import get_collections
from app.database.enums import ChatRoleType
from app.database.models import (
    ChatHistoryModel,
    ChatSessionModel,
    SettingModel,
    UserProfileModel,
    HealthRecordModel
)
from app.dto.models import ChatHistoryModelDTO
from utils import get_logger
from ..create_chat_command import CreateChatCommand
from shared.rag_templates import render_template

dotenv.load_dotenv()

@CommandRegistry.register_handler(CreateChatCommand)
class CreateChatCommandHandler(CommandHandler):
    """Handler for processing CreateChatCommand to manage chat sessions and responses."""
    def __init__(self):
        super().__init__()
        self.logger = get_logger(__name__)
        self.db = get_collections()
        self.llm_client = None
        self.retriever_cache = {}
        self.LLM_TIMEOUT = 75  # Increased for complex queries
        self.DB_TIMEOUT = 15
        self.TOTAL_TIMEOUT = 120

    async def get_llm_client(self) -> QwenLLM:
        """Initialize or return cached QwenLLM client."""
        if self.llm_client is None:
            self.llm_client = QwenLLM(
                model=os.getenv("QWEN_MODEL", "qwen2.5:3b-instruct"),
                base_url=os.getenv("QWEN_URL", "http://localhost:11434")
            )
        return self.llm_client

    async def _with_timeout(self, coro, timeout_seconds: int, operation_name: str):
        """Execute coroutine with timeout and logging."""
        try:
            start_time = asyncio.get_event_loop().time()
            result = await asyncio.wait_for(coro, timeout=timeout_seconds)
            elapsed = asyncio.get_event_loop().time() - start_time
            self.logger.debug(f"{operation_name} completed in {elapsed:.2f}s")
            return result
        except asyncio.TimeoutError:
            self.logger.error(f"TIMEOUT: {operation_name} exceeded {timeout_seconds}s")
            raise asyncio.TimeoutError(f"{operation_name} timeout after {timeout_seconds}s")
        except Exception as e:
            elapsed = asyncio.get_event_loop().time() - start_time
            self.logger.error(f"{operation_name} failed after {elapsed:.2f}s: {e}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
    async def create_session(
        self,
        user_id: str,
        title: str,
        session_id: Optional[str] = None
    ) -> Optional[ChatSessionModel]:
        """Create or retrieve a chat session."""
        if not user_id or not title:
            self.logger.error("Invalid user_id or title")
            return None
        try:
            if user_id == "admin":
                doc = await self._with_timeout(
                    self.db.chat_sessions.find_one({"user_id": "admin"}),
                    self.DB_TIMEOUT,
                    "Find Admin Session"
                )
                if doc:
                    return ChatSessionModel.from_dict(doc)
                session = ChatSessionModel(user_id="admin", title="Test AI")
                result = await self._with_timeout(
                    self.db.chat_sessions.insert_one(session.to_dict()),
                    self.DB_TIMEOUT,
                    "Insert Admin Session"
                )
                session._id = result.inserted_id
                return session

            if session_id:
                try:
                    obj_id = ObjectId(session_id)
                    doc = await self._with_timeout(
                        self.db.chat_sessions.find_one({"_id": obj_id}),
                        self.DB_TIMEOUT,
                        "Find Session by ID"
                    )
                    if doc:
                        return ChatSessionModel.from_dict(doc)
                except ValueError:
                    self.logger.error(f"Invalid session_id: {session_id}")
                    return None

            session_title = title[:100] + "..." if len(title) > 100 else title
            session = ChatSessionModel(user_id=user_id, title=session_title)
            result = await self._with_timeout(
                self.db.chat_sessions.insert_one(session.to_dict()),
                self.DB_TIMEOUT,
                "Insert New Session"
            )
            session._id = result.inserted_id
            return session

        except Exception as e:
            self.logger.error(f"Error creating session: {e}", exc_info=True)
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
    async def update_session(self, session_id: str) -> bool:
        """Update session's last modified timestamp."""
        try:
            obj_id = ObjectId(session_id)
            result = await self._with_timeout(
                self.db.chat_sessions.update_one(
                    {"_id": obj_id},
                    {"$set": {"updated_at": datetime.utcnow()}}
                ),
                self.DB_TIMEOUT,
                "Update Session"
            )
            return result.modified_count > 0
        except (ValueError, Exception) as e:
            self.logger.error(f"Update session failed: {e}", exc_info=True)
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
    async def get_histories(self, session_id: str) -> List[ChatHistoryModel]:
        """Retrieve recent chat history for a session."""
        try:
            obj_id = ObjectId(session_id)
            cursor = self.db.chat_histories.find({"session_id": str(obj_id)}) \
                .sort("updated_at", -1).limit(20)
            docs = await self._with_timeout(
                cursor.to_list(length=20),
                self.DB_TIMEOUT,
                "Get Chat Histories"
            )
            histories = []
            for doc in docs:
                model = ChatHistoryModel.from_dict(doc)
                if isinstance(model.role, str):
                    model.role = ChatRoleType.USER if model.role.lower() == "user" else ChatRoleType.AI
                histories.append(model)
            return histories
        except (ValueError, Exception) as e:
            self.logger.error(f"Cannot get chat history: {e}", exc_info=True)
            return []

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
    async def save_data(self, data: ChatHistoryModel) -> bool:
        """Save chat history to database."""
        if not data or not data.session_id or not data.content:
            self.logger.error("Invalid chat history data")
            return False
        try:
            if isinstance(data.session_id, ObjectId):
                data.session_id = str(data.session_id)
            result = await self._with_timeout(
                self.db.chat_histories.insert_one(data.to_dict()),
                self.DB_TIMEOUT,
                "Save Chat History"
            )
            return result.acknowledged
        except Exception as e:
            self.logger.error(f"Save chat history failed: {e}", exc_info=True)
            return False

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
    async def get_user_profile(self, user_id: str) -> Optional[UserProfileModel]:
        """Retrieve user profile from database."""
        if not user_id:
            self.logger.error("Invalid user_id")
            return None
        try:
            doc = await self._with_timeout(
                self.db.user_profiles.find_one({"user_id": user_id}),
                self.DB_TIMEOUT,
                "Get User Profile"
            )
            return UserProfileModel.from_dict(doc) if doc else None
        except Exception as e:
            self.logger.error(f"Kh√¥ng th·ªÉ l·∫•y h·ªì s∆° ng∆∞·ªùi d√πng {user_id}: {e}")
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=5))
    async def get_recent_health_records(self, user_id: str, record_type: str, top: int = 3) -> List[HealthRecordModel]:
        """Retrieve recent health records for a user."""
        if not user_id or not record_type:
            self.logger.error(f"Invalid user_id or record_type: {user_id}, {record_type}")
            return []
        try:
            cursor = self.db.health_records.find({
                "user_id": user_id,
                "type": record_type
            }).sort("timestamp", -1).limit(top)
            docs = await self._with_timeout(
                cursor.to_list(length=top),
                self.DB_TIMEOUT,
                f"Get {record_type} Records"
            )
            return [HealthRecordModel.from_dict(doc) for doc in docs if doc]
        except Exception as e:
            self.logger.error(f"L·ªói l·∫•y ch·ªâ s·ªë {record_type}: {e}")
            return []

    async def get_relevant_user_context(self, user_id: str, question: str) -> str:
        """Generate relevant user context based on question."""
        profile = await self.get_user_profile(user_id)
        if not profile or user_id == "admin":
            return "B√°c ∆°i, tui ch∆∞a c√≥ th√¥ng tin v·ªÅ b√°c. H√£y c·∫≠p nh·∫≠t h·ªì s∆° ƒë·ªÉ tui h·ªó tr·ª£ nh√©!"

        parts = [
            f"B√°c {profile.full_name} (ID: {profile.patient_id}), {profile.age} tu·ªïi, {profile.gender}, "
            f"ƒëang qu·∫£n l√Ω ti·ªÉu ƒë∆∞·ªùng lo·∫°i {profile.diabetes_type}."
        ]
        q = question.lower()

        if any(kw in q for kw in ["ƒë∆∞·ªùng huy·∫øt", "glucose", "ch·ªâ s·ªë ƒë∆∞·ªùng"]):
            records = await self.get_recent_health_records(user_id, "ƒê∆∞·ªùng huy·∫øt", top=3)
            if records:
                summary = ", ".join([f"{r.value:.1f} mmol/L ({r.timestamp.strftime('%d/%m')})" for r in records])
                parts.append(f"ƒê∆∞·ªùng huy·∫øt g·∫ßn ƒë√¢y: {summary}.")
            else:
                parts.append("ƒê∆∞·ªùng huy·∫øt: ch∆∞a c√≥ d·ªØ li·ªáu g·∫ßn ƒë√¢y.")
            if profile.complications:
                parts.append(f"Bi·∫øn ch·ª©ng hi·ªán t·∫°i: {', '.join(profile.complications)}.")

        if any(kw in q for kw in ["huy·∫øt √°p", "blood pressure", "tim m·∫°ch"]):
            records = await self.get_recent_health_records(user_id, "Huy·∫øt √°p", top=3)
            if records:
                sys = [r.value for r in records if r.subtype == "T√¢m thu"]
                dia = [r.value for r in records if r.subtype == "T√¢m tr∆∞∆°ng"]
                if sys and dia:
                    parts.append(f"Huy·∫øt √°p g·∫ßn ƒë√¢y: trung b√¨nh {sum(sys)/len(sys):.0f}/{sum(dia)/len(dia):.0f} mmHg.")
                else:
                    parts.append("Huy·∫øt √°p: d·ªØ li·ªáu kh√¥ng ƒë·∫ßy ƒë·ªß.")
            else:
                parts.append("Huy·∫øt √°p: ch∆∞a c√≥ d·ªØ li·ªáu g·∫ßn ƒë√¢y.")

        if any(kw in q for kw in ["insulin", "ti√™m"]):
            if profile.insulin_schedule:
                parts.append(f"L·ªãch ti√™m insulin: {profile.insulin_schedule}.")

        if any(kw in q for kw in ["ƒÉn", "ch·∫ø ƒë·ªô", "l·ªëi s·ªëng"]):
            if profile.lifestyle:
                parts.append(f"L·ªëi s·ªëng: {profile.lifestyle}.")
            if profile.bmi:
                parts.append(f"BMI: {profile.bmi:.1f}.")

        return "\n".join(parts) or "Tui c·∫ßn th√™m th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c h∆°n, b√°c chia s·∫ª th√™m nh√©!"

    async def classify_question_type(self, question: str, histories: List[ChatHistoryModel]) -> Dict[str, any]:
        """Classify question type using LLM with context from history."""
        llm = await self.get_llm_client()
        history_text = "\n".join([
            f"- {msg.role}: {msg.content}"
            for msg in histories[-3:] if msg.content
        ]) if histories else "Kh√¥ng c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán."

        prompt = f"""
B·∫°n l√† h·ªá th·ªëng ph√¢n lo·∫°i c√¢u h·ªèi y t·∫ø chuy√™n v·ªÅ b·ªánh ti·ªÉu ƒë∆∞·ªùng v√† s·ª©c kh·ªèe li√™n quan. 
Nhi·ªám v·ª•: Ph√¢n lo·∫°i c√¢u h·ªèi th√†nh **ch√≠nh x√°c 1 lo·∫°i** t·ª´ c√°c lo·∫°i sau:
- `greeting`: L·ªùi ch√†o nh∆∞ "xin ch√†o", "ch√†o b·∫°n", "hello".
- `invalid`: C√¢u h·ªèi ti√™u c·ª±c, t·ª± t·ª≠, b·ªè ƒëi·ªÅu tr·ªã ("ch·∫øt", "b·ªè thu·ªëc", "m·ªát qu√°").
- `personal_info`: H·ªèi v·ªÅ ch·ªâ s·ªë, thu·ªëc, bi·∫øn ch·ª©ng, ch·∫ø ƒë·ªô ƒÉn c·ªßa b·∫£n th√¢n ("c·ªßa t√¥i", "t√¨nh tr·∫°ng t√¥i").
- `trend_analysis`: H·ªèi v·ªÅ xu h∆∞·ªõng, ch·ªâ s·ªë g·∫ßn ƒë√¢y ("g·∫ßn ƒë√¢y", "xu h∆∞·ªõng", "c√≥ ·ªïn kh√¥ng").
- `relational`: So s√°nh ti·ªÉu ƒë∆∞·ªùng v·ªõi b·ªánh kh√°c ("ung th∆∞", "tr·∫ßm c·∫£m").
- `rag_only`: Ki·∫øn th·ª©c chung v·ªÅ b·ªánh, nguy√™n nh√¢n, lo·∫°i b·ªánh ("c√≥ m·∫•y lo·∫°i", "l√† g√¨").

**Tr·∫£ v·ªÅ**: Ch·ªâ **1 t·ª´** (greeting, invalid, personal_info, trend_analysis, relational, rag_only), kh√¥ng gi·∫£i th√≠ch, kh√¥ng vi·∫øt hoa.

**L∆∞u √Ω**:
- D√πng l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh.
- ∆Øu ti√™n `rag_only` cho c√¢u h·ªèi ki·∫øn th·ª©c chung v·ªÅ ti·ªÉu ƒë∆∞·ªùng.
- Kh√¥ng nh·∫ßm "ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng" v·ªõi "personal_info" n·∫øu kh√¥ng c√≥ chia s·∫ª c√° nh√¢n.
- C√¢u h·ªèi v·ªÅ "ch·ªâ s·ªë g·∫ßn ƒë√¢y" ho·∫∑c "c√≥ ·ªïn kh√¥ng" thu·ªôc `trend_analysis`.

**V√≠ d·ª•**:
- "B·ªánh ti·ªÉu ƒë∆∞·ªùng c√≥ m·∫•y lo·∫°i v·∫≠y" ‚Üí rag_only
- "V·∫≠y c√≤n c√°c ch·ªâ s·ªë g·∫ßn ƒë√¢y tui c√≥ ·ªïn kh√¥ng" ‚Üí trend_analysis
- "B·ªánh ung th∆∞ c√≥ m·∫•y lo·∫°i v·∫≠y" ‚Üí rag_only
- "ƒê∆∞·ªùng huy·∫øt c·ªßa t√¥i th·∫ø n√†o" ‚Üí personal_info
- "Ch√†o b·∫°n" ‚Üí greeting
- "T√¥i m·ªát qu√°, s·ªëng l√†m g√¨" ‚Üí invalid
- "Ti·ªÉu ƒë∆∞·ªùng li√™n quan g√¨ ƒë·∫øn ung th∆∞" ‚Üí relational

**C√¢u h·ªèi hi·ªán t·∫°i**: "{question}"
**L·ªãch s·ª≠ tr√≤ chuy·ªán**: 
{history_text}

**Lo·∫°i**:
""".strip()

        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt, max_tokens=20, temperature=0.01),
                self.LLM_TIMEOUT,
                "Question Classification"
            )
            response = response.strip().lower()
            valid_types = ["greeting", "invalid", "personal_info", "trend_analysis", "relational", "rag_only"]
            return {"type": response} if response in valid_types else {"type": "rag_only"}
        except Exception as e:
            self.logger.error(f"LLM classification failed: {e}")
            q = question.lower()
            if any(kw in q for kw in ["ch·∫øt", "b·ªè thu·ªëc", "m·ªát qu√°"]):
                return {"type": "invalid"}
            if any(kw in q for kw in ["ch√†o", "hello", "hi "]):
                return {"type": "greeting"}
            if any(kw in q for kw in ["g·∫ßn ƒë√¢y", "xu h∆∞·ªõng", "c√≥ ·ªïn kh√¥ng"]):
                return {"type": "trend_analysis"}
            if any(kw in q for kw in ["so v·ªõi", "li√™n quan", "ung th∆∞", "tr·∫ßm c·∫£m"]):
                return {"type": "relational"}
            if any(phrase in q for phrase in ["c·ªßa t√¥i", "t√¨nh tr·∫°ng t√¥i", "l·ªãch ti√™m c·ªßa t√¥i"]):
                return {"type": "personal_info"}
            return {"type": "rag_only"}

    def _is_valid_context(self, content: str) -> bool:
        """Validate context content to avoid code or irrelevant data."""
        text = content.strip().lower()
        if len(text) < 30:
            return False
        suspicious = ["import ", "def ", "class ", "from ", "os.", "dotenv", "error", "exception", "traceback"]
        return not any(s in text for s in suspicious)

    async def _retrieve_rag_context(self, query: str, settings: SettingModel) -> List[str]:
        """Retrieve relevant context from vector store."""
        if not settings.list_knowledge_ids:
            self.logger.info("No knowledge IDs configured for RAG")
            return []
        try:
            retrieval_result: Result = await self._with_timeout(
                Mediator.send(GetRetrievedContextQuery(query=query)),
                30,
                "RAG Retrieval"
            )
            if retrieval_result.is_success and retrieval_result.data:
                contexts = [
                    dto.content for dto in retrieval_result.data
                    if dto.content and self._is_valid_context(dto.content)
                ]
                self.logger.debug(f"Found {len(contexts)} valid contexts for query: '{query}'")
                return contexts[:settings.top_k]
            self.logger.info(f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ƒë·ªß li√™n quan cho: '{query}'")
            return []
        except Exception as e:
            self.logger.error(f"RAG retrieval failed: {e}")
            return []

    async def _gen_rag_only_response(self, message: str, contexts: List[str], histories: List[ChatHistoryModel]) -> str:
        """Generate response for general knowledge questions."""
        if not contexts:
            q = message.lower()
            if any(kw in q for kw in ["c√≥ m·∫•y lo·∫°i", "ngo√†i lo·∫°i 1 v√† 2"]):
                return (
                    "**B·ªánh ti·ªÉu ƒë∆∞·ªùng c√≥ c√°c lo·∫°i ch√≠nh**:\n\n"
                    "1. **Lo·∫°i 1**: C∆° th·ªÉ kh√¥ng s·∫£n xu·∫•t insulin do h·ªá mi·ªÖn d·ªãch t·∫•n c√¥ng t·∫ø b√†o beta.\n"
                    "2. **Lo·∫°i 2**: C∆° th·ªÉ kh√°ng insulin ho·∫∑c kh√¥ng s·∫£n xu·∫•t ƒë·ªß insulin.\n"
                    "3. **Ti·ªÉu ƒë∆∞·ªùng thai k·ª≥**: X·∫£y ra trong thai k·ª≥, th∆∞·ªùng t·ª± h·∫øt sau sinh.\n"
                    "4. **MODY v√† c√°c lo·∫°i hi·∫øm**: Do gen, √≠t g·∫∑p.\n\n"
                    "Lo·∫°i 1 v√† lo·∫°i 2 chi·∫øm ph·∫ßn l·ªõn (~95%) c√°c tr∆∞·ªùng h·ª£p. B√°c mu·ªën bi·∫øt th√™m v·ªÅ lo·∫°i n√†o kh√¥ng?"
                )
            if "ung th∆∞" in q:
                return (
                    "B√°c ∆°i, hi·ªán tui ch·ªâ chuy√™n v·ªÅ **b·ªánh ti·ªÉu ƒë∆∞·ªùng** v√† c√°c v·∫•n ƒë·ªÅ li√™n quan nh∆∞ ƒë∆∞·ªùng huy·∫øt, insulin, ch·∫ø ƒë·ªô ƒÉn. "
                    "V·ªÅ ung th∆∞, tui ch∆∞a c√≥ ƒë·ªß th√¥ng tin ch√≠nh x√°c ƒë·ªÉ tr·∫£ l·ªùi. "
                    "B√°c c√≥ th·ªÉ h·ªèi v·ªÅ **ti·ªÉu ƒë∆∞·ªùng** ho·∫∑c chia s·∫ª th√™m ƒë·ªÉ tui h·ªó tr·ª£ nha!"
                )
            return (
                "B√°c ∆°i, tui ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p cho c√¢u h·ªèi n√†y. üòÖ\n\n"
                "H√£y h·ªèi v·ªÅ **ƒë∆∞·ªùng huy·∫øt, insulin, ch·∫ø ƒë·ªô ƒÉn u·ªëng** ho·∫∑c b·ªánh ti·ªÉu ƒë∆∞·ªùng, tui s·∫Ω tr·∫£ l·ªùi ngay!"
            )

        try:
            with open("shared/rag_templates/system_prompt.txt", "r", encoding="utf-8") as f:
                system_prompt = f.read().strip()
        except Exception:
            system_prompt = "B·∫°n l√† chuy√™n gia y t·∫ø v·ªÅ b·ªánh ti·ªÉu ƒë∆∞·ªùng, tr·∫£ l·ªùi r√µ r√†ng, th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát, d√πng Markdown."

        full_context = "\n\n---\n\n".join([
            f"[T√ÄI LI·ªÜU {i+1}]\n{ctx.strip()}" for i, ctx in enumerate(contexts)
        ]) if contexts else "Kh√¥ng c√≥ t√†i li·ªáu li√™n quan."

        try:
            prompt_text = render_template(
                template_name="rag_only.j2",
                system_prompt=system_prompt,
                contexts=full_context,
                question=message,
                histories=histories
            )
        except Exception as e:
            self.logger.error(f"Template rendering failed: {e}")
            return "√îi, tui g·∫∑p ch√∫t tr·ª•c tr·∫∑c khi t·∫°o c√¢u tr·∫£ l·ªùi. B√°c h·ªèi l·∫°i nha!"

        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=400, temperature=0.7),
                self.LLM_TIMEOUT,
                "RAG Response Generation"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return "√îi, tui b·ªã k·∫πt ch√∫t r·ªìi, b√°c th·ª≠ h·ªèi l·∫°i nha!"
        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")
            return "Tui g·∫∑p v·∫•n ƒë·ªÅ nh·ªè, b√°c h·ªèi l·∫°i ƒë·ªÉ tui h·ªó tr·ª£ ti·∫øp nh√©!"

    async def _gen_personalized_response(self, message: str, contexts: List[str], user_context: str, user_id: str, first_time: bool, histories: List[ChatHistoryModel]) -> str:
        """Generate personalized response based on user context."""
        profile = await self.get_user_profile(user_id)
        if not profile:
            return "B√°c ∆°i, tui ch∆∞a c√≥ th√¥ng tin h·ªì s∆° c·ªßa b√°c. H√£y c·∫≠p nh·∫≠t ƒë·ªÉ tui h·ªó tr·ª£ nha!"

        try:
            with open("shared/rag_templates/system_prompt.txt", "r", encoding="utf-8") as f:
                system_prompt = f.read().strip()
        except Exception:
            system_prompt = "B·∫°n l√† b√°c sƒ© n·ªôi ti·∫øt, tr·∫£ l·ªùi th√¢n thi·ªán, r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát, d√πng Markdown."

        full_context = "\n\n---\n\n".join([
            f"[T√ÄI LI·ªÜU {i+1}]\n{ctx.strip()}" for i, ctx in enumerate(contexts)
        ]) if contexts else "Kh√¥ng c√≥ t√†i li·ªáu li√™n quan."

        history_context = ""
        if not first_time:
            history_context = (
                "L·∫ßn tr∆∞·ªõc tui ƒë√£ xem qua t√¨nh tr·∫°ng c·ªßa b√°c r·ªìi. " if any("xu h∆∞·ªõng" in msg.content.lower() for msg in histories if msg.role == ChatRoleType.AI)
                else "B√°c ƒë√£ h·ªèi tui tr∆∞·ªõc ƒë√≥, gi·ªù tui s·∫Ω tr·∫£ l·ªùi chi ti·∫øt h∆°n nha."
            )

        try:
            prompt_text = render_template(
                template_name="personalized.j2",
                system_prompt=system_prompt,
                contexts=full_context,
                user_context=user_context,
                question=message,
                full_name=profile.full_name,
                age=profile.age,
                first_time=first_time,
                history_context=history_context,
                histories=histories
            )
        except Exception as e:
            self.logger.error(f"Template personalized.j2 rendering failed: {e}")
            return "√îi, tui g·∫∑p ch√∫t tr·ª•c tr·∫∑c khi t·∫°o c√¢u tr·∫£ l·ªùi. B√°c h·ªèi l·∫°i nha!"

        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=500, temperature=0.7),
                self.LLM_TIMEOUT,
                "Personalized Response Generation"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return f"Ch√†o {'b√°c' if profile.age >= 50 else 'anh/ch·ªã'} {profile.full_name}, tui ƒëang x·ª≠ l√Ω ch·∫≠m ch√∫t. B√°c h·ªèi l·∫°i nha!"
        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")
            return "Tui g·∫∑p v·∫•n ƒë·ªÅ nh·ªè, b√°c h·ªèi l·∫°i ƒë·ªÉ tui h·ªó tr·ª£ ti·∫øp nh√©!"

    async def get_polite_response_for_invalid_question(self, question: str) -> str:
        """Generate polite response for invalid questions."""
        try:
            prompt_text = render_template(
                template_name="polite_response.j2",
                question=question
            )
        except Exception:
            return (
                "B√°c ∆°i, tui hi·ªÉu b√°c c√≥ th·ªÉ ƒëang m·ªát m·ªèi, nh∆∞ng **s·ª©c kh·ªèe r·∫•t quan tr·ªçng**! üòä\n\n"
                "H√£y chia s·∫ª th√™m v·ªÅ t√¨nh tr·∫°ng c·ªßa b√°c ho·∫∑c h·ªèi v·ªÅ **ƒë∆∞·ªùng huy·∫øt, insulin**, tui s·∫Ω h·ªó tr·ª£ ngay. "
                "N·∫øu c·∫ßn, b√°c n√™n g·∫∑p b√°c sƒ© ho·∫∑c ng∆∞·ªùi th√¢n ƒë·ªÉ ƒë∆∞·ª£c gi√∫p ƒë·ª° th√™m nha!"
            )
        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=300, temperature=0.7),
                self.LLM_TIMEOUT,
                "Polite Response"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return (
                "B√°c ∆°i, s·ª©c kh·ªèe c·ªßa b√°c r·∫•t quan tr·ªçng! H√£y t√¨m h·ªó tr·ª£ t·ª´ b√°c sƒ© ho·∫∑c ng∆∞·ªùi th√¢n nha, tui lu√¥n ·ªü ƒë√¢y ƒë·ªÉ gi√∫p!"
            )
        except Exception:
            return (
                "B√°c ∆°i, s·ª©c kh·ªèe c·ªßa b√°c r·∫•t quan tr·ªçng! H√£y t√¨m h·ªó tr·ª£ t·ª´ b√°c sƒ© ho·∫∑c ng∆∞·ªùi th√¢n nha, tui lu√¥n ·ªü ƒë√¢y ƒë·ªÉ gi√∫p!"
            )

    def _ensure_markdown(self, text: str) -> str:
        """Ensure response is clean and in valid Markdown format."""
        if not text.strip():
            return "B√°c ∆°i, tui ch∆∞a t√¨m ra c√¢u tr·∫£ l·ªùi ph√π h·ª£p. H·ªèi l·∫°i nha!"
        
        lower_text = text.lower()
        if any(kw in lower_text for kw in ["import ", "def ", "class ", "from ", "os.", "dotenv", "b√†i tr·∫£ l·ªùi", "tu√¢n th·ªß"]):
            return (
                "Tui ch∆∞a c√≥ th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y. üòÖ\n\n"
                "B√°c h·ªèi v·ªÅ ƒë∆∞·ªùng huy·∫øt, huy·∫øt √°p, hay ch·∫ø ƒë·ªô ƒÉn u·ªëng ƒëi, tui s·∫Ω tr·∫£ l·ªùi ngay!"
            )

        import re
        text = re.sub(r'\*\*(.*?)\*\*', r'**\1**', text)
        text = re.sub(r'\*(.*?)\*', r'*\1*', text)

        lines = text.split('\n')
        cleaned = []
        in_leak = False
        leak_keywords = ["h√£y suy nghƒ©", "ph√¢n t√≠ch", "t√¥i c·∫ßn tr·∫£ l·ªùi", "let me think", "step by step", "b√†i tr·∫£ l·ªùi", "tu√¢n th·ªß"]
        for line in lines:
            lower_line = line.lower()
            if any(kw in lower_line for kw in leak_keywords):
                in_leak = True
                continue
            if line.startswith("### ") and in_leak:
                in_leak = False
                cleaned.append(line)
            elif not in_leak and line.strip():
                cleaned.append(line)
        result = '\n'.join(cleaned).strip()

        if result and (result.startswith(("I ", "You ")) or "cannot" in result.lower() or "sorry" in result.lower()):
            return "Xin l·ªói, tui ch·ªâ h·ªó tr·ª£ b·∫±ng ti·∫øng Vi·ªát, b√°c h·ªèi l·∫°i nha!"
        return result if result else "Tui ch∆∞a c√≥ th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, b√°c h·ªèi th√™m chi ti·∫øt nha!"

    async def execute(self, command: CreateChatCommand) -> Result[None]:
        """Execute the CreateChatCommand to process user query and generate response."""
        if not command or not command.user_id or not command.content:
            self.logger.error("Invalid command data")
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message="D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá, vui l√≤ng ki·ªÉm tra l·∫°i."
            )
        try:
            return await self._with_timeout(
                self._execute_internal(command),
                self.TOTAL_TIMEOUT,
                "Complete Chat Processing"
            )
        except asyncio.TimeoutError as e:
            self.logger.error(f"Total execution timeout: {e}")
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message="√îi, tui x·ª≠ l√Ω h∆°i l√¢u, b√°c th·ª≠ l·∫°i nha!"
            )
        except Exception as e:
            self.logger.error(f"Error in _execute_internal: {e}", exc_info=True)
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message="Tui g·∫∑p v·∫•n ƒë·ªÅ nh·ªè, b√°c th·ª≠ l·∫°i nha!"
            )

    async def _execute_internal(self, command: CreateChatCommand) -> Result[None]:
        """Internal execution logic for chat command."""
        try:
            settings_doc = await self._with_timeout(
                self.db.settings.find_one({}),
                self.DB_TIMEOUT,
                "Get Settings"
            )
            if not settings_doc:
                return Result.failure(
                    code=SettingMessage.NOT_FOUND.code,
                    message=SettingMessage.NOT_FOUND.message
                )
            settings = SettingModel.from_dict(settings_doc)

            session = await self.create_session(
                user_id=command.user_id,
                title=command.content,
                session_id=command.session_id
            )
            if not session:
                return Result.failure(message="Kh√¥ng t·∫°o ƒë∆∞·ª£c session.")

            user_chat = ChatHistoryModel(
                session_id=str(session.id),
                user_id=command.user_id,
                content=command.content,
                role=ChatRoleType.USER
            )
            if not await self.save_data(user_chat):
                return Result.failure(message="Kh√¥ng l∆∞u ƒë∆∞·ª£c tin nh·∫Øn ng∆∞·ªùi d√πng.")

            histories = await self.get_histories(session.id)
            histories.reverse()

            ai_messages = [msg for msg in histories if msg.role == ChatRoleType.AI]
            first_time = len(ai_messages) == 0
            has_previous_trend = any(
                kw in msg.content.lower()
                for kw in ["xu h∆∞·ªõng", "g·∫ßn ƒë√¢y", "ƒë√°nh gi√°", "ph√¢n t√≠ch", "thay ƒë·ªïi"]
                for msg in ai_messages
            )

            contexts = await self._retrieve_rag_context(command.content, settings)
            self.logger.info(f"RAG Retrieval: found {len(contexts)} contexts")

            classification = await self.classify_question_type(command.content, histories)
            question_type = classification["type"]
            self.logger.info(f"üîç Question: '{command.content}' ‚Üí Type: {question_type}")

            gen_text = ""
            if question_type == "greeting":
                profile = await self.get_user_profile(command.user_id)
                name = profile.full_name if profile else "b·∫°n"
                gen_text = (
                    f"Ch√†o {'b√°c' if profile and profile.age >= 50 else 'anh/ch·ªã'} {name}! üòä\n\n"
                    "Tui l√† tr·ª£ l√Ω y t·∫ø chuy√™n v·ªÅ ti·ªÉu ƒë∆∞·ªùng. B√°c mu·ªën h·ªèi g√¨ h√¥m nay? V√≠ d·ª• nh∆∞ ƒë∆∞·ªùng huy·∫øt, insulin, hay ch·∫ø ƒë·ªô ƒÉn u·ªëng n√®!"
                )
            elif question_type == "invalid":
                gen_text = await self.get_polite_response_for_invalid_question(command.content)
            elif question_type == "trend_analysis":
                gen_text = await self._gen_personalized_response(
                    message=command.content,
                    contexts=contexts,
                    user_context=await self.get_relevant_user_context(command.user_id, command.content),
                    user_id=command.user_id,
                    first_time=first_time,
                    histories=histories
                )
            elif question_type == "personal_info":
                gen_text = await self._gen_personalized_response(
                    message=command.content,
                    contexts=contexts,
                    user_context=await self.get_relevant_user_context(command.user_id, command.content),
                    user_id=command.user_id,
                    first_time=first_time,
                    histories=histories
                )
            elif question_type == "relational":
                gen_text = await self._gen_rag_only_response(command.content, contexts, histories)
            else:
                gen_text = await self._gen_rag_only_response(command.content, contexts, histories)

            ai_chat = ChatHistoryModel(
                session_id=str(session.id),
                user_id=command.user_id,
                content=gen_text,
                role=ChatRoleType.AI
            )
            if not await self.save_data(ai_chat):
                return Result.failure(message="Kh√¥ng l∆∞u ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi AI.")

            await self.update_session(session.id)

            dto = ChatHistoryModelDTO.from_model(ai_chat)
            return Result.success(
                code=ChatMessage.CHAT_CREATED.code,
                message=ChatMessage.CHAT_CREATED.message,
                data=dto
            )

        except Exception as e:
            self.logger.error(f"Error in _execute_internal: {e}", exc_info=True)
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message="Tui g·∫∑p v·∫•n ƒë·ªÅ nh·ªè, b√°c th·ª≠ l·∫°i nha!"
            )