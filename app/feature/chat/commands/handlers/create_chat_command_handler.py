import os
import dotenv
import asyncio
from datetime import datetime
from typing import List, Optional
from bson import ObjectId

from core.cqrs import CommandRegistry, CommandHandler
from core.embedding import EmbeddingModel
from core.llm import QwenLLM
from core.result import Result
from rag.vector_store import VectorStoreManager
from rag.retrieval.retriever import Retriever
from shared.messages import ChatMessage, SettingMessage
from app.database import get_collections
from app.database.enums import ChatRoleType
from app.database.models import (
    ChatHistoryModel,
    ChatSessionModel,
    SettingModel,
    UserProfileModel,
    HealthRecordModel
)
from app.dto.models import ChatHistoryModelDTO
from utils import get_logger
from ..create_chat_command import CreateChatCommand
from shared.rag_templates import render_template

dotenv.load_dotenv()

@CommandRegistry.register_handler(CreateChatCommand)
class CreateChatCommandHandler(CommandHandler):
    def __init__(self):
        super().__init__()
        self.logger = get_logger(__name__)
        self.db = get_collections()
        self.vector_store_manager = VectorStoreManager()
        self.embedding_model = None
        self.llm_client = None
        self.retriever_cache = {}
        
        self.RAG_TIMEOUT = 30
        self.LLM_TIMEOUT = 45  
        self.EMBEDDING_TIMEOUT = 20 
        self.DB_TIMEOUT = 10 
        self.TOTAL_TIMEOUT = 120

    async def get_llm_client(self) -> QwenLLM:
        if self.llm_client is None:
            self.llm_client = QwenLLM(
                model=os.getenv("QWEN_MODEL", "qwen2.5:3b-instruct"),
                base_url=os.getenv("QWEN_URL", "http://localhost:11434")
            )
        return self.llm_client

    async def get_embedding_model(self) -> EmbeddingModel:
        if self.embedding_model is None:
            self.embedding_model = EmbeddingModel()
        return self.embedding_model
    
    async def _with_timeout(self, coro, timeout_seconds: int, operation_name: str):
        try:
            start_time = asyncio.get_event_loop().time()
            result = await asyncio.wait_for(coro, timeout=timeout_seconds)
            elapsed = asyncio.get_event_loop().time() - start_time
            self.logger.debug(f"{operation_name} completed in {elapsed:.2f}s")
            return result
        except asyncio.TimeoutError:
            self.logger.error(f"TIMEOUT: {operation_name} exceeded {timeout_seconds}s")
            raise asyncio.TimeoutError(f"{operation_name} timeout after {timeout_seconds}s")
        except Exception as e:
            elapsed = asyncio.get_event_loop().time() - start_time
            self.logger.error(f"{operation_name} failed after {elapsed:.2f}s: {e}")
            raise

    async def create_session(
        self,
        user_id: str,
        title: str,
        session_id: str = None
    ) -> Optional[ChatSessionModel]:
        try:
            if user_id == "admin":
                doc = await self.db.chat_sessions.find_one({"user_id": "admin"})
                if doc:
                    return ChatSessionModel.from_dict(doc)
                session = ChatSessionModel(user_id="admin", title="Test AI")
                result = await self.db.chat_sessions.insert_one(session.to_dict())
                session._id = result.inserted_id
                return session

            if session_id:
                obj_id = ObjectId(session_id)
                doc = await self.db.chat_sessions.find_one({"_id": obj_id})
                if doc:
                    return ChatSessionModel.from_dict(doc)

            session_title = title[:100] + "..." if len(title) > 100 else title
            session = ChatSessionModel(user_id=user_id, title=session_title)
            result = await self.db.chat_sessions.insert_one(session.to_dict())
            session._id = result.inserted_id
            return session

        except Exception as e:
            self.logger.error(f"Error creating session: {e}", exc_info=True)
            return None

    async def update_session(self, session_id: str) -> bool:
        try:
            obj_id = ObjectId(session_id)
            result = await self.db.chat_sessions.update_one(
                {"_id": obj_id},
                {"$set": {"updated_at": datetime.utcnow()}}
            )
            return result.modified_count > 0
        except Exception as e:
            self.logger.error(f"Update session failed: {e}", exc_info=True)
            return False

    async def get_histories(self, session_id: str) -> List[ChatHistoryModel]:
        try:
            obj_id = ObjectId(session_id)
            cursor = self.db.chat_histories.find({"session_id": str(obj_id)}) \
                .sort("updated_at", -1).limit(20)
            docs = await cursor.to_list(length=20)
            histories = []
            for doc in docs:
                model = ChatHistoryModel.from_dict(doc)
                if isinstance(model.role, str):
                    model.role = ChatRoleType.USER if model.role.lower() == "user" else ChatRoleType.AI
                histories.append(model)
            return histories
        except Exception as e:
            self.logger.error(f"Cannot get chat history: {e}", exc_info=True)
            return []

    async def save_data(self, data: ChatHistoryModel) -> bool:
        try:
            if isinstance(data.session_id, ObjectId):
                data.session_id = str(data.session_id)
            result = await self.db.chat_histories.insert_one(data.to_dict())
            return result.acknowledged
        except Exception as e:
            self.logger.error(f"Save chat history failed: {e}", exc_info=True)
            return False

    def get_retriever(self, collections: List[str]) -> Retriever:
        key = ",".join(sorted(collections))
        if key not in self.retriever_cache:
            self.retriever_cache[key] = Retriever(
                collections=collections,
                vector_store_manager=self.vector_store_manager
            )
        return self.retriever_cache[key]

    async def get_user_profile(self, user_id: str) -> Optional[UserProfileModel]:
        try:
            doc = await self.db.user_profiles.find_one({"user_id": user_id})
            return UserProfileModel.from_dict(doc) if doc else None
        except Exception as e:
            self.logger.error(f"Kh√¥ng th·ªÉ l·∫•y h·ªì s∆° ng∆∞·ªùi d√πng {user_id}: {e}")
            return None

    async def get_recent_health_records(self, user_id: str, record_type: str, top: int = 3) -> List[HealthRecordModel]:
        try:
            cursor = self.db.health_records.find({
                "user_id": user_id,
                "type": record_type
            }).sort("timestamp", -1).limit(top)
            docs = await cursor.to_list(length=top)
            return [HealthRecordModel.from_dict(doc) for doc in docs if doc]
        except Exception as e:
            self.logger.error(f"L·ªói l·∫•y ch·ªâ s·ªë: {e}")
            return []

    async def get_relevant_user_context(self, user_id: str, question: str) -> str:
        profile = await self.get_user_profile(user_id)
        if not profile or user_id == "admin":
            return ""
        q = question.lower()
        parts = [
            f"B·ªánh nh√¢n: {profile.full_name} (ID: {profile.patient_id}), {profile.age} tu·ªïi, {profile.gender}, "
            f"ti·ªÉu ƒë∆∞·ªùng lo·∫°i {profile.diabetes_type}"
        ]
        if any(kw in q for kw in ["ƒë∆∞·ªùng huy·∫øt", "glucose"]):
            records = await self.get_recent_health_records(user_id, "BloodGlucose", top=3)
            if records:
                summary = ", ".join([f"{r.value:.1f} mmol/l" for r in records])
                parts.append(f"ƒê∆∞·ªùng huy·∫øt: {summary}")
            if profile.complications:
                parts.append(f"Bi·∫øn ch·ª©ng: {', '.join(profile.complications)}")
        if any(kw in q for kw in ["huy·∫øt √°p", "blood pressure"]):
            records = await self.get_recent_health_records(user_id, "BloodPressure", top=2)
            if records:
                sys = [r.value for r in records if r.subtype == "t√¢m thu"]
                if sys:
                    parts.append(f"Huy·∫øt √°p: trung b√¨nh {sum(sys)/len(sys):.0f} mmHg")
        if any(kw in q for kw in ["insulin", "ti√™m"]):
            if profile.insulin_schedule:
                parts.append(f"L·ªãch ti√™m insulin: {profile.insulin_schedule}")
        if any(kw in q for kw in ["ƒÉn", "ch·∫ø ƒë·ªô", "l·ªëi s·ªëng"]):
            if profile.lifestyle:
                parts.append(f"L·ªëi s·ªëng: {profile.lifestyle}")
            if profile.bmi:
                parts.append(f"BMI: {profile.bmi:.1f}")
        return "\n".join(parts)

    async def classify_question_type(self, question: str) -> str:
        llm = await self.get_llm_client()
        prompt = f"""
B·∫°n l√† h·ªá th·ªëng ph√¢n lo·∫°i c√¢u h·ªèi y t·∫ø t·ª± ƒë·ªông. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n lo·∫°i c√¢u h·ªèi ng∆∞·ªùi d√πng v√†o ƒë√∫ng 1 trong 4 lo·∫°i: `rag_only`, `personal`, `trend`, `invalid`.

Ch·ªâ ƒë∆∞·ª£c tr·∫£ v·ªÅ **1 t·ª´ duy nh·∫•t**: rag_only, personal, trend, ho·∫∑c invalid.
Kh√¥ng gi·∫£i th√≠ch, kh√¥ng th√™m k√Ω t·ª±, kh√¥ng vi·∫øt hoa.

---

### üîç B∆Ø·ªöC 1: X√ÅC ƒê·ªäNH C√ì PH·∫¢I C√ÇU H·ªéI NGUY HI·ªÇM?
Ki·ªÉm tra xem c√¢u h·ªèi c√≥ ch·ª©a n·ªôi dung ti√™u c·ª±c, t·ª± t·ª≠, b·ªè ƒëi·ªÅu tr·ªã kh√¥ng:
- T·ª´ kh√≥a: "ch·∫øt", "b·ªè thu·ªëc", "m·ªát qu√°", "s·ªëng l√†m g√¨", "kh√¥ng c·∫ßn ki·ªÉm so√°t"
‚Üí N·∫øu C√ì ‚Üí tr·∫£ v·ªÅ: `invalid`

---

### üîç B∆Ø·ªöC 2: X√ÅC ƒê·ªäNH C√ì PH·∫¢I THEO D√ïI XU H∆Ø·ªöNG?
Ki·ªÉm tra t·ª´ li√™n quan ƒë·∫øn th·ªùi gian, so s√°nh:
- T·ª´ kh√≥a: "g·∫ßn ƒë√¢y", "xu h∆∞·ªõng", "3 th√°ng qua", "so v·ªõi tu·∫ßn tr∆∞·ªõc", "thay ƒë·ªïi th·∫ø n√†o", "d·∫°o n√†y"
‚Üí N·∫øu C√ì ‚Üí tr·∫£ v·ªÅ: `trend`

---

### üîç B∆Ø·ªöC 3: X√ÅC ƒê·ªäNH C√ì PH·∫¢I CHIA S·∫∫ C√Å NH√ÇN?
Ki·ªÉm tra xem ng∆∞·ªùi h·ªèi c√≥ chia s·∫ª t√¨nh tr·∫°ng, ch·ªâ s·ªë, tri·ªáu ch·ª©ng c√° nh√¢n kh√¥ng:
- T·ª´ kh√≥a: "t√¥i b·ªã", "c·ªßa t√¥i", "t√¨nh tr·∫°ng c·ªßa t√¥i", "ƒë∆∞·ªùng huy·∫øt c·ªßa t√¥i", "huy·∫øt √°p t√¥i", "b√°c sƒ© n√≥i t√¥i"
‚Üí N·∫øu C√ì ‚Üí tr·∫£ v·ªÅ: `personal`

---

### üîç B∆Ø·ªöC 4: C√ÇU H·ªéI KI·∫æN TH·ª®C CHUNG?
N·∫øu kh√¥ng thu·ªôc 3 lo·∫°i tr√™n, d√π c√≥ d√πng "t√¥i mu·ªën bi·∫øt", "ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng n√™n ƒÉn g√¨", "c√≥ m·∫•y lo·∫°i", "l√† g√¨":
‚Üí Tr·∫£ v·ªÅ: `rag_only`

---

### üìè LU·∫¨T R√ï R√ÄNG
- rag_only: C√¢u h·ªèi v·ªÅ ki·∫øn th·ª©c y h·ªçc chung, kh√¥ng li√™n quan ƒë·∫øn ng∆∞·ªùi h·ªèi
- personal: Ng∆∞·ªùi h·ªèi ƒëang chia s·∫ª b·∫£n th√¢n, c√≥ ch·ªâ s·ªë, tri·ªáu ch·ª©ng
- trend: C√≥ y·∫øu t·ªë th·ªùi gian, so s√°nh, ƒë√°nh gi√° thay ƒë·ªïi
- invalid: Nguy hi·ªÉm, ti√™u c·ª±c, t·ª± t·ª≠

---

### ‚úÖ V√ç D·ª§ CHU·∫®N
- "Ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng n√™n ƒÉn g√¨?" ‚Üí rag_only
- "T√¥i b·ªã ti·ªÉu ƒë∆∞·ªùng 5 nƒÉm r·ªìi, n√™n ƒÉn g√¨?" ‚Üí personal
- "ƒê∆∞·ªùng huy·∫øt g·∫ßn ƒë√¢y c·ªßa t√¥i th·∫ø n√†o?" ‚Üí trend
- "L√†m sao ƒë·ªÉ ch·∫øt nhanh?" ‚Üí invalid
- "Bi·∫øn ch·ª©ng ti·ªÉu ƒë∆∞·ªùng g·ªìm nh·ªØng g√¨?" ‚Üí rag_only
- "T√¥i m·ªát qu√°, s·ªëng l√†m g√¨?" ‚Üí invalid
- "Huy·∫øt √°p d·∫°o n√†y ra sao?" ‚Üí trend
- "Insulin ho·∫°t ƒë·ªông trong bao l√¢u?" ‚Üí rag_only

---

### ‚ùå L∆ØU √ù QUAN TR·ªåNG
- Kh√¥ng ph√¢n lo·∫°i nh·∫ßm "ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng" th√†nh "c√° nh√¢n"
- Kh√¥ng coi "t√¥i mu·ªën bi·∫øt" l√† "personal" n·∫øu kh√¥ng c√≥ chia s·∫ª
- Kh√¥ng tr·∫£ v·ªÅ nhi·ªÅu t·ª´, kh√¥ng vi·∫øt th√™m

---

C√¢u h·ªèi: "{question}"
""".strip()

        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt, max_tokens=20, temperature=0.05),
                self.LLM_TIMEOUT,
                "Question Classification"
            )
            response = response.strip().lower()
            if response in ["rag_only", "personal", "trend", "invalid"]:
                return response
            if any(kw in response for kw in ["ki·∫øn th·ª©c", "chung", "l√† g√¨", "c√≥ m·∫•y lo·∫°i"]):
                return "rag_only"
            if any(kw in response for kw in ["t√¥i b·ªã", "c·ªßa t√¥i", "t√¨nh tr·∫°ng"]):
                return "personal"
            return "rag_only"
        except Exception as e:
            self.logger.error(f"Classification failed: {e}")
            return "rag_only"

    def _is_valid_context(self, content: str) -> bool:
        text = content.strip().lower()
        if len(text) < 30:
            return False
        suspicious = ["import ", "def ", "class ", "from ", "os.", "dotenv", "error", "exception", "traceback", "not found", "file not found"]
        if any(s in text for s in suspicious):
            return False
        return True

    async def _retrieve_rag_context(self, query: str, histories: List[ChatHistoryModel], settings: SettingModel) -> List[str]:
        if not settings.list_knowledge_ids:
            return []

        try:
            embedding = await self.get_embedding_model()
            query_vector = await self._with_timeout(
                embedding.embed(query),
                self.EMBEDDING_TIMEOUT,
                "Query Embedding"
            )
            
            retriever = self.get_retriever(settings.list_knowledge_ids)
            results = await self._with_timeout(
                retriever.retrieve(query_vector, top_k=settings.top_k * 2),
                self.RAG_TIMEOUT,
                "Vector Search"
            )

            score_threshold = getattr(settings, "search_accuracy", 0.75)
            filtered = [hit for hit in results if hit["score"] >= score_threshold]

            contexts = [
                hit["payload"]["content"]
                for hit in filtered
                if hit["payload"] and hit["payload"].get("content")
                and self._is_valid_context(hit["payload"]["content"])
            ][:settings.top_k]
            
            if not contexts:
                self.logger.info(f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ƒë·ªß li√™n quan cho: '{query}'")
                return []

            self.logger.debug(f"Found {len(contexts)} valid contexts for query: '{query}'")
            return contexts

        except asyncio.TimeoutError as e:
            self.logger.error(f"RAG retrieval timeout: {e}")
            return []
        except Exception as e:
            self.logger.error(f"RAG retrieval failed: {e}")
            return []

    async def _gen_rag_only_response(self, message: str, contexts: List[str], histories: List[ChatHistoryModel]) -> str:
        if not contexts:
            return (
                "**Hi·ªán t√¥i ch∆∞a c√≥ t√†i li·ªáu** li√™n quan ƒë·∫øn c√¢u h·ªèi n√†y.\n\n"
                "N·∫øu b·∫°n c√≥ c√¢u h·ªèi v·ªÅ **ƒë∆∞·ªùng huy·∫øt, insulin, ch·∫ø ƒë·ªô ƒÉn cho ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng**, "
                "t√¥i r·∫•t s·∫µn l√≤ng h·ªó tr·ª£.\n\n"
                "B·∫°n c≈©ng c√≥ th·ªÉ cung c·∫•p th√™m chi ti·∫øt ƒë·ªÉ t√¥i t√¨m hi·ªÉu k·ªπ h∆°n."
            )

        try:
            with open("shared/rag_templates/system_prompt.txt", "r", encoding="utf-8") as f:
                system_prompt = f.read().strip()
        except Exception as e:
            system_prompt = "B·∫°n l√† chuy√™n gia y t·∫ø, tr·∫£ l·ªùi r√µ r√†ng, d√πng Markdown."

        cleaned_contexts = "\n\n".join([
            ctx.strip()
            .replace("[HEADING]", "### ").replace("[/HEADING]", "\n")
            .replace("[SUBHEADING]", "#### ").replace("[/SUBHEADING]", "\n")
            for ctx in contexts if ctx.strip()
        ])

        try:
            prompt_text = render_template(
                template_name="rag_only.j2",
                system_prompt=system_prompt,
                contexts=cleaned_contexts,
                question=message,
                histories=histories
            )
        except Exception as e:
            return "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."

        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=1800),
                self.LLM_TIMEOUT,
                "RAG Response Generation"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return "Xin l·ªói, t√¥i ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau."
        except Exception as e:
            return "Xin l·ªói, t√¥i ƒëang b·∫≠n."

    async def _gen_personalized_response(self, message: str, contexts: List[str], user_context: str, user_id: str, first_time: bool = True, histories: List[ChatHistoryModel] = None) -> str:
        profile = await self.get_user_profile(user_id)
        if not profile:
            return "Kh√¥ng t√¨m th·∫•y h·ªì s∆° ng∆∞·ªùi d√πng."
        full_name = profile.full_name
        age = profile.age

        try:
            with open("shared/rag_templates/system_prompt.txt", "r", encoding="utf-8") as f:
                system_prompt = f.read().strip()
        except Exception as e:
            system_prompt = "B·∫°n l√† b√°c sƒ© n·ªôi ti·∫øt."

        cleaned_contexts = "\n\n".join([ctx.strip() for ctx in contexts if ctx.strip()])

        try:
            prompt_text = render_template(
                template_name="personalized.j2",
                system_prompt=system_prompt,
                contexts=cleaned_contexts,
                user_context=user_context,
                question=message,
                full_name=full_name,
                age=age,
                first_time=first_time,
                histories=histories
            )
        except Exception as e:
            return "Xin l·ªói, kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."

        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=1800),
                self.LLM_TIMEOUT,
                "RAG Response Generation"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return "Xin l·ªói, t√¥i ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau."
        except Exception as e:
            return "Xin l·ªói, t√¥i ƒëang b·∫≠n."

    async def _analyze_blood_glucose_only(self, user_id: str, question: str, first_time: bool = True, has_previous_trend: bool = False) -> str:
        profile = await self.get_user_profile(user_id)
        if not profile:
            return "Kh√¥ng t√¨m th·∫•y h·ªì s∆° ng∆∞·ªùi d√πng."
        records = await self.get_recent_health_records(user_id, "BloodGlucose", top=3)
        if not records:
            return "Ch∆∞a c√≥ d·ªØ li·ªáu ƒë∆∞·ªùng huy·∫øt ƒë·ªÉ ƒë√°nh gi√°."
        values = [r.value for r in records]
        avg = sum(values) / len(values)
        latest = records[0].value
        trend = "tƒÉng" if len(values) >= 2 and values[0] > values[-1] else "gi·∫£m" if len(values) >= 2 and values[0] < values[-1] else "·ªïn ƒë·ªãnh"
        status = "cao" if avg > 8.0 else "trung b√¨nh" if avg > 6.0 else "th·∫•p"
        health_summary = f"ƒê∆∞·ªùng huy·∫øt: trung b√¨nh {avg:.1f} mmol/l, g·∫ßn nh·∫•t {latest:.1f} mmol/l ‚Äî m·ª©c {status}, xu h∆∞·ªõng {trend}"
        user_context = await self.get_relevant_user_context(user_id, question)

        history_context = ""
        if not first_time:
            if has_previous_trend:
                history_context = "L·∫ßn tr∆∞·ªõc, t√¥i ƒë√£ ph√¢n t√≠ch xu h∆∞·ªõng cho b√°c."
            else:
                history_context = "B√°c ƒë√£ h·ªèi tr∆∞·ªõc ƒë√≥, nh∆∞ng ch∆∞a ph√¢n t√≠ch xu h∆∞·ªõng."

        try:
            prompt_text = render_template(
                template_name="trend_response.j2",
                user_context=user_context,
                health_summary=health_summary,
                question=question,
                full_name=profile.full_name,
                age=profile.age,
                first_time=first_time,
                has_previous_trend=has_previous_trend,
                history_context=history_context
            )
        except Exception as e:
            return "Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi chi ti·∫øt."
        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=500),
                self.LLM_TIMEOUT,
                "Health Analysis Response"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return "Xin l·ªói, t√¥i ƒëang x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i sau."
        except Exception as e:
            return "Xin l·ªói, t√¥i ƒëang x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i sau."

    async def _analyze_blood_pressure_only(self, user_id: str, question: str, first_time: bool = True, has_previous_trend: bool = False) -> str:
        profile = await self.get_user_profile(user_id)
        if not profile:
            return "Kh√¥ng t√¨m th·∫•y h·ªì s∆° ng∆∞·ªùi d√πng."
        records = await self.get_recent_health_records(user_id, "BloodPressure", top=3)
        if not records:
            return "Ch∆∞a c√≥ d·ªØ li·ªáu huy·∫øt √°p ƒë·ªÉ ƒë√°nh gi√°."
        systolic = [r.value for r in records if r.subtype == "t√¢m thu"]
        if not systolic:
            return "Kh√¥ng c√≥ d·ªØ li·ªáu huy·∫øt √°p t√¢m thu."
        avg_sys = sum(systolic) / len(systolic)
        avg_dia = sum([r.value for r in records if r.subtype == "t√¢m tr∆∞∆°ng"]) / len([r for r in records if r.subtype == "t√¢m tr∆∞∆°ng"])
        if avg_sys > 140 or avg_dia > 90:
            bp_status = "cao ‚Äì nguy c∆° tim m·∫°ch tƒÉng"
        elif avg_sys > 120 or avg_dia > 80:
            bp_status = "bi√™n ƒë·ªô cao ‚Äì c·∫ßn theo d√µi"
        else:
            bp_status = "·ªïn ƒë·ªãnh"
        health_summary = f"Huy·∫øt √°p: trung b√¨nh {avg_sys:.0f}/{avg_dia:.0f} mmHg ‚Äî m·ª©c {bp_status}"
        user_context = await self.get_relevant_user_context(user_id, question)

        history_context = ""
        if not first_time:
            if has_previous_trend:
                history_context = "L·∫ßn tr∆∞·ªõc, t√¥i ƒë√£ ph√¢n t√≠ch huy·∫øt √°p cho b√°c."
            else:
                history_context = "B√°c ƒë√£ h·ªèi tr∆∞·ªõc ƒë√≥, nh∆∞ng ch∆∞a ph√¢n t√≠ch xu h∆∞·ªõng."

        try:
            prompt_text = render_template(
                template_name="trend_response.j2",
                user_context=user_context,
                health_summary=health_summary,
                question=question,
                full_name=profile.full_name,
                age=profile.age,
                first_time=first_time,
                has_previous_trend=has_previous_trend,
                history_context=history_context
            )
        except Exception as e:
            return "Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi chi ti·∫øt."
        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=500),
                self.LLM_TIMEOUT,
                "Blood Pressure Analysis"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return "Xin l·ªói, t√¥i ƒëang x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i."
        except Exception as e:
            return "Xin l·ªói, t√¥i ƒëang x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i."

    async def _analyze_overall_status(self, user_id: str, question: str, first_time: bool = True, has_previous_trend: bool = False) -> str:
        profile = await self.get_user_profile(user_id)
        if not profile:
            return "Kh√¥ng t√¨m th·∫•y h·ªì s∆° ng∆∞·ªùi d√πng."
        glucose_records = await self.get_recent_health_records(user_id, "BloodGlucose", top=3)
        bp_records = await self.get_recent_health_records(user_id, "BloodPressure", top=3)
        parts = []
        if glucose_records:
            values = [r.value for r in glucose_records]
            avg = sum(values) / len(values)
            status = "cao" if avg > 8.0 else "trung b√¨nh"
            parts.append(f"ƒê∆∞·ªùng huy·∫øt: trung b√¨nh {avg:.1f} mmol/l ‚Üí m·ª©c {status}")
        if bp_records:
            sys = [r.value for r in bp_records if r.subtype == "t√¢m thu"]
            if sys:
                avg_sys = sum(sys) / len(sys)
                bp_status = "cao" if avg_sys > 140 else "bi√™n ƒë·ªô cao" if avg_sys > 120 else "·ªïn ƒë·ªãnh"
                parts.append(f"Huy·∫øt √°p: trung b√¨nh {avg_sys:.0f} mmHg ‚Üí m·ª©c {bp_status}")
        if not parts:
            return "Ch∆∞a c√≥ d·ªØ li·ªáu s·ª©c kh·ªèe g·∫ßn ƒë√¢y ƒë·ªÉ ƒë√°nh gi√°."
        health_summary = "\n".join(parts)
        user_context = await self.get_relevant_user_context(user_id, question)

        history_context = ""
        if not first_time:
            if has_previous_trend:
                history_context = "L·∫ßn tr∆∞·ªõc, t√¥i ƒë√£ t·ªïng h·ª£p t√¨nh tr·∫°ng s·ª©c kh·ªèe cho b√°c."
            else:
                history_context = "B√°c ƒë√£ h·ªèi tr∆∞·ªõc ƒë√≥, nh∆∞ng ch∆∞a ph√¢n t√≠ch t·ªïng qu√°t."

        try:
            prompt_text = render_template(
                template_name="trend_response.j2",
                user_context=user_context,
                health_summary=health_summary,
                question=question,
                full_name=profile.full_name,
                age=profile.age,
                first_time=first_time,
                has_previous_trend=has_previous_trend,
                history_context=history_context
            )
        except Exception as e:
            return "Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi chi ti·∫øt."
        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=600),
                self.LLM_TIMEOUT,
                "Overall Health Analysis"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return "Xin l·ªói, t√¥i ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i."
        except Exception as e:
            return "Xin l·ªói, t√¥i ƒëang b·∫≠n."

    async def generate_health_status_response(
        self,
        user_id: str,
        question: str,
        first_time: bool = True,
        has_previous_trend: bool = False
    ) -> str:
        profile = await self.get_user_profile(user_id)
        if not profile:
            try:
                prompt_text = render_template(
                    template_name="no_profile_response.j2",
                    question=question
                )
            except Exception as e:
                return (
                    "**T√¥i hi·ªÉu** r·∫±ng vi·ªác chia s·∫ª th√¥ng tin c√° nh√¢n c√≥ th·ªÉ khi·∫øn b·∫°n c·∫£m th·∫•y kh√¥ng tho·∫£i m√°i, "
                    "nh∆∞ng ƒë·ªÉ t√¥i h·ªó tr·ª£ b·∫°n t·ªët nh·∫•t, b·∫°n vui l√≤ng c·∫≠p nh·∫≠t m·ªôt s·ªë th√¥ng tin c∆° b·∫£n nh∆∞ tu·ªïi v√† lo·∫°i b·ªánh l√Ω.\n\n"
                    "Ch·ªâ c·∫ßn v√†i ph√∫t th·ªùi gian ‚Äî s·∫Ω gi√∫p t√¥i ƒë∆∞a ra l·ªùi khuy√™n **ch√≠nh x√°c v√† ph√π h·ª£p v·ªõi ho√†n c·∫£nh c·ªßa b·∫°n**.\n\n"
                    "B·∫°n kh√¥ng ƒë∆°n ƒë·ªôc trong h√†nh tr√¨nh n√†y ‚Äî t√¥i lu√¥n ·ªü ƒë√¢y ƒë·ªÉ ƒë·ªìng h√†nh."
                )
            llm = await self.get_llm_client()
            try:
                response = await self._with_timeout(
                    llm.generate(prompt=prompt_text, max_tokens=500),
                    self.LLM_TIMEOUT,
                    "No Profile Response"
                )
                return self._ensure_markdown(response.strip())
            except asyncio.TimeoutError:
                return (
                    "**T√¥i hi·ªÉu** r·∫±ng vi·ªác chia s·∫ª th√¥ng tin c√° nh√¢n c√≥ th·ªÉ khi·∫øn b·∫°n c·∫£m th·∫•y kh√¥ng tho·∫£i m√°i, "
                    "nh∆∞ng ƒë·ªÉ t√¥i h·ªó tr·ª£ b·∫°n t·ªët nh·∫•t, b·∫°n vui l√≤ng c·∫≠p nh·∫≠t m·ªôt s·ªë th√¥ng tin c∆° b·∫£n nh∆∞ tu·ªïi v√† lo·∫°i b·ªánh l√Ω.\n\n"
                    "Ch·ªâ c·∫ßn v√†i ph√∫t th·ªùi gian ‚Äî s·∫Ω gi√∫p t√¥i ƒë∆∞a ra l·ªùi khuy√™n **ch√≠nh x√°c v√† ph√π h·ª£p v·ªõi ho√†n c·∫£nh c·ªßa b·∫°n**.\n\n"
                    "B·∫°n kh√¥ng ƒë∆°n ƒë·ªôc trong h√†nh tr√¨nh n√†y ‚Äî t√¥i lu√¥n ·ªü ƒë√¢y ƒë·ªÉ ƒë·ªìng h√†nh."
                )
            except Exception as e:
                return (
                    "**T√¥i hi·ªÉu** r·∫±ng vi·ªác chia s·∫ª th√¥ng tin c√° nh√¢n c√≥ th·ªÉ khi·∫øn b·∫°n c·∫£m th·∫•y kh√¥ng tho·∫£i m√°i, "
                    "nh∆∞ng ƒë·ªÉ t√¥i h·ªó tr·ª£ b·∫°n t·ªët nh·∫•t, b·∫°n vui l√≤ng c·∫≠p nh·∫≠t m·ªôt s·ªë th√¥ng tin c∆° b·∫£n nh∆∞ tu·ªïi v√† lo·∫°i b·ªánh l√Ω.\n\n"
                    "Ch·ªâ c·∫ßn v√†i ph√∫t th·ªùi gian ‚Äî s·∫Ω gi√∫p t√¥i ƒë∆∞a ra l·ªùi khuy√™n **ch√≠nh x√°c v√† ph√π h·ª£p v·ªõi ho√†n c·∫£nh c·ªßa b·∫°n**.\n\n"
                    "B·∫°n kh√¥ng ƒë∆°n ƒë·ªôc trong h√†nh tr√¨nh n√†y ‚Äî t√¥i lu√¥n ·ªü ƒë√¢y ƒë·ªÉ ƒë·ªìng h√†nh."
                )

        glucose_records = await self.get_recent_health_records(user_id, "BloodGlucose", top=1)
        bp_records = await self.get_recent_health_records(user_id, "BloodPressure", top=1)

        if not glucose_records and not bp_records:
            try:
                prompt_text = render_template(
                    template_name="no_data_response.j2",
                    question=question,
                    full_name=profile.full_name
                )
            except Exception as e:
                return (
                    "**Hi·ªán t√¥i ch∆∞a th·∫•y** c√≥ d·ªØ li·ªáu s·ª©c kh·ªèe g·∫ßn ƒë√¢y.\n\n"
                    "H√£y b·∫Øt ƒë·∫ßu **ghi l·∫°i ƒë∆∞·ªùng huy·∫øt 1‚Äì2 l·∫ßn m·ªói ng√†y**."
                )
            llm = await self.get_llm_client()
            try:
                response = await self._with_timeout(
                    llm.generate(prompt=prompt_text, max_tokens=500),
                    self.LLM_TIMEOUT,
                    "No Data Response"
                )
                return self._ensure_markdown(response.strip())
            except asyncio.TimeoutError:
                return (
                    "**Ch√∫ng ta ch∆∞a c√≥** d·ªØ li·ªáu g·∫ßn ƒë√¢y.\n\n"
                    "H√£y th·ª≠ **ƒëo v√† ghi l·∫°i** ‚Äì t√¥i s·∫Ω gi√∫p b·∫°n ph√¢n t√≠ch ngay khi c√≥ s·ªë li·ªáu."
                )
            except Exception as e:
                return (
                    "**Ch√∫ng ta ch∆∞a c√≥** d·ªØ li·ªáu g·∫ßn ƒë√¢y.\n\n"
                    "H√£y th·ª≠ **ƒëo v√† ghi l·∫°i** ‚Äì t√¥i s·∫Ω gi√∫p b·∫°n ph√¢n t√≠ch ngay khi c√≥ s·ªë li·ªáu."
                )

        q = question.lower()
        if "huy·∫øt √°p" in q:
            return await self._analyze_blood_pressure_only(user_id, question, first_time, has_previous_trend)
        elif any(kw in q for kw in ["ƒë∆∞·ªùng huy·∫øt", "glucose"]):
            if not glucose_records:
                try:
                    prompt_text = render_template(
                        template_name="no_glucose_data.j2",
                        question=question,
                        full_name=profile.full_name
                    )
                except Exception as e:
                    return "**Ch∆∞a c√≥ d·ªØ li·ªáu** ƒë∆∞·ªùng huy·∫øt."
                llm = await self.get_llm_client()
                try:
                    response = await self._with_timeout(
                        llm.generate(prompt=prompt_text, max_tokens=500),
                        self.LLM_TIMEOUT,
                        "No Glucose Data Response"
                    )
                    return self._ensure_markdown(response.strip())
                except asyncio.TimeoutError:
                    return "H√£y b·∫Øt ƒë·∫ßu ƒëo ƒë∆∞·ªùng huy·∫øt m·ªói ng√†y."
                except Exception as e:
                    return "H√£y b·∫Øt ƒë·∫ßu ƒëo ƒë∆∞·ªùng huy·∫øt m·ªói ng√†y."
            return await self._analyze_blood_glucose_only(user_id, question, first_time, has_previous_trend)
        else:
            return await self._analyze_overall_status(user_id, question, first_time, has_previous_trend)

    async def get_polite_response_for_invalid_question(self, question: str) -> str:
        try:
            prompt_text = render_template(
                template_name="polite_response.j2",
                question=question
            )
        except Exception as e:
            return (
                "**T√¥i hi·ªÉu** b·∫°n c√≥ th·ªÉ ƒëang c·∫£m th·∫•y m·ªát m·ªèi, nh∆∞ng **s·ª©c kh·ªèe c·ªßa b·∫°n r·∫•t quan tr·ªçng**.\n\n"
                "H√£y t√¨m s·ª± h·ªó tr·ª£ t·ª´ b√°c sƒ© ho·∫∑c ng∆∞·ªùi th√¢n ‚Äì b·∫°n kh√¥ng ƒë∆°n ƒë·ªôc."
            )
        llm = await self.get_llm_client()
        try:
            response = await self._with_timeout(
                llm.generate(prompt=prompt_text, max_tokens=500),
                self.LLM_TIMEOUT,
                "Polite Response"
            )
            return self._ensure_markdown(response.strip())
        except asyncio.TimeoutError:
            return (
                "**S·ª©c kh·ªèe c·ªßa b·∫°n r·∫•t quan tr·ªçng**. H√£y t√¨m s·ª± h·ªó tr·ª£ ‚Äî b·∫°n kh√¥ng ƒë∆°n ƒë·ªôc."
            )
        except Exception as e:
            return (
                "**S·ª©c kh·ªèe c·ªßa b·∫°n r·∫•t quan tr·ªçng**. H√£y t√¨m s·ª± h·ªó tr·ª£ ‚Äî b·∫°n kh√¥ng ƒë∆°n ƒë·ªôc."
            )

    def _ensure_markdown(self, text: str) -> str:
        if not text.strip():
            return "T√¥i ch∆∞a th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p."

        lower_text = text.lower()
        if any(kw in lower_text for kw in ["import ", "def ", "class ", "from ", "os.", "dotenv"]):
            return (
                "**Hi·ªán t√¥i ch∆∞a c√≥ t√†i li·ªáu** li√™n quan ƒë·∫øn c√¢u h·ªèi n√†y.\n\n"
                "H√£y h·ªèi v·ªÅ **ƒë∆∞·ªùng huy·∫øt, insulin, ch·∫ø ƒë·ªô ƒÉn** ƒë·ªÉ t√¥i h·ªó tr·ª£ t·ªët h∆°n."
            )

        import re
        text = re.sub(r'\*\*(.*?)\*\*', r'**\1**', text)
        text = re.sub(r'\*(.*?)\*', r'*\1*', text)

        lines = text.split('\n')
        cleaned = []
        in_leak = False
        leak_keywords = ["h√£y suy nghƒ©", "ph√¢n t√≠ch", "t√¥i c·∫ßn tr·∫£ l·ªùi", "let me think", "step by step"]
        for line in lines:
            lower_line = line.lower()
            if any(kw in lower_line for kw in leak_keywords):
                in_leak = True
                continue
            if line.startswith("### ") and in_leak:
                in_leak = False
                cleaned.append(line)
            elif not in_leak and line.strip():
                cleaned.append(line)
        result = '\n'.join(cleaned).strip()

        if result and (result.startswith(("I ", "You ")) or "cannot" in result or "Sorry" in result):
            return "Xin l·ªói, t√¥i ch·ªâ h·ªó tr·ª£ b·∫±ng ti·∫øng Vi·ªát."
        return result if result else "T√¥i ch∆∞a c√≥ th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi."

    async def execute(self, command: CreateChatCommand) -> Result[None]:
        try:
            return await self._with_timeout(
                self._execute_internal(command),
                self.TOTAL_TIMEOUT,
                "Complete Chat Processing"
            )
        except asyncio.TimeoutError as e:
            self.logger.error(f"Total execution timeout: {e}")
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message="Xin l·ªói, x·ª≠ l√Ω qu√° l√¢u. Vui l√≤ng th·ª≠ l·∫°i."
            )
        except Exception as e:
            self.logger.error(f"Error in _execute_internal: {e}", exc_info=True)
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message=ChatMessage.CHAT_CREATED_FAILED.message
            )
    
    async def _execute_internal(self, command: CreateChatCommand) -> Result[None]:
        try:
            settings_doc = await self.db.settings.find_one({})
            if not settings_doc:
                return Result.failure(
                    code=SettingMessage.NOT_FOUND.code,
                    message=SettingMessage.NOT_FOUND.message
                )
            settings = SettingModel.from_dict(settings_doc)

            session = await self.create_session(
                user_id=command.user_id,
                title=command.content,
                session_id=command.session_id
            )
            if not session:
                return Result.failure(message="Kh√¥ng t·∫°o ƒë∆∞·ª£c session.")

            user_chat = ChatHistoryModel(
                session_id=str(session.id),
                user_id=command.user_id,
                content=command.content,
                role=ChatRoleType.USER
            )
            await self.save_data(user_chat)

            histories = await self.get_histories(session.id)
            histories.reverse()

            ai_messages = [msg for msg in histories if msg.role == ChatRoleType.AI]
            first_time = len(ai_messages) == 0

            has_previous_trend = any(
                "xu h∆∞·ªõng" in msg.content.lower() or
                "g·∫ßn ƒë√¢y" in msg.content.lower() or
                "ƒë√°nh gi√°" in msg.content.lower() or
                "ph√¢n t√≠ch" in msg.content.lower()
                for msg in ai_messages
            )

            content_lower = command.content.lower()

            question_type = await self.classify_question_type(command.content)

            gen_text = ""

            if question_type == "invalid":
                gen_text = await self.get_polite_response_for_invalid_question(command.content)
            elif question_type == "trend":
                gen_text = await self.generate_health_status_response(command.user_id, command.content, first_time, has_previous_trend)
            elif question_type == "rag_only":
                context_texts = await self._retrieve_rag_context(command.content, histories, settings)
                gen_text = await self._gen_rag_only_response(command.content, context_texts, histories)
            elif question_type == "personal":
                context_texts = await self._retrieve_rag_context(command.content, histories, settings)
                user_context = await self.get_relevant_user_context(command.user_id, command.content)
                if context_texts and user_context:
                    gen_text = await self._gen_personalized_response(command.content, context_texts, user_context, command.user_id, first_time, histories)
                elif context_texts:
                    gen_text = await self._gen_rag_only_response(command.content, context_texts, histories)
                else:
                    health_keywords = ["ƒë∆∞·ªùng huy·∫øt", "huy·∫øt √°p", "ti·ªÉu ƒë∆∞·ªùng", "insulin"]
                    if any(kw in content_lower for kw in health_keywords):
                        gen_text = await self.generate_health_status_response(command.user_id, command.content, first_time, has_previous_trend)
                    else:
                        gen_text = (
                            "**T√¥i hi·ªÉu** b·∫°n mu·ªën t√¨m hi·ªÉu th√™m.\n\n"
                            f"Hi·ªán t√¥i ch∆∞a c√≥ t√†i li·ªáu v·ªÅ **\"{command.content}\"**.\n\n"
                            "N·∫øu b·∫°n c√≥ c√¢u h·ªèi v·ªÅ **ƒë∆∞·ªùng huy·∫øt, insulin, ch·∫ø ƒë·ªô ƒÉn**, "
                            "t√¥i r·∫•t s·∫µn l√≤ng h·ªó tr·ª£."
                        )
            else:
                context_texts = await self._retrieve_rag_context(command.content, histories, settings)
                gen_text = await self._gen_rag_only_response(command.content, context_texts, histories)

            gen_text = self._ensure_markdown(gen_text)

            ai_chat = ChatHistoryModel(
                session_id=str(session.id),
                user_id=command.user_id,
                content=gen_text,
                role=ChatRoleType.AI
            )
            await self.save_data(ai_chat)
            await self.update_session(session.id)

            dto = ChatHistoryModelDTO.from_model(ai_chat)
            return Result.success(
                code=ChatMessage.CHAT_CREATED.code,
                message=ChatMessage.CHAT_CREATED.message,
                data=dto
            )

        except Exception as e:
            self.logger.error(f"Error in _execute_internal: {e}", exc_info=True)
            return Result.failure(
                code=ChatMessage.CHAT_CREATED_FAILED.code,
                message=ChatMessage.CHAT_CREATED_FAILED.message
            )