"""
RAG API routes cho knowledge base management v√† document processing
"""

import os
import tempfile
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, Union

from fastapi import APIRouter, File, UploadFile, HTTPException, Form, Query, status
from fastapi.responses import JSONResponse

from features.rag.rag_pipeline import RAGPipeline, RAGPipelineConfig
from features.rag.chunking import ChunkingConfig
from features.rag.embedding import EmbeddingConfig, Embedding
from features.rag.vector_store import VectorStoreConfig, VectorStore
from features.rag.storage import document_storage
from features.rag.retrieval import (
    HybridRetriever,
    MultiCollectionConfig,
    create_multi_collection_retriever,
    HybridSearchConfig,
    MultiCollectionHybridRetriever,
)
from core.logging_config import get_logger
from .models import (
    KnowledgeBaseCreate,
    KnowledgeBaseResponse,
    KnowledgeBaseList,
    FileUploadResponse,
    FileInfoModel,
    CollectionStats,
    MultiCollectionSearchRequest,
    MultiCollectionSearchResponse,
    SearchResult,
)
from utils.utils import validate_retriever_initialization, check_retriever_type
from features.rag.document_parser import DocumentParser

router = APIRouter(tags=["RAG Knowledge Base"])
logger = get_logger(__name__)

SUPPORTED_EXTENSIONS = {".pdf", ".docx", ".txt", ".html", ".htm", ".csv", ".md"}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB

# Singleton pipeline
rag_pipeline: RAGPipeline | None = None


def get_rag_pipeline(collection_name: str) -> RAGPipeline:
    """Get RAG pipeline instance v·ªõi collection name c·ª• th·ªÉ."""
    global rag_pipeline
    if not rag_pipeline:
        config = RAGPipelineConfig(
            chunking_config=ChunkingConfig(),
            embedding_config=EmbeddingConfig(),
            vector_store_config=VectorStoreConfig(collection_name=collection_name),
        )
        rag_pipeline = RAGPipeline(config)
    else:
        # Ki·ªÉm tra xem collection ƒë√£ t·ªìn t·∫°i ch∆∞a
        if collection_name not in rag_pipeline.vector_stores:
            # T·∫°o vector store m·ªõi cho collection
            config = VectorStoreConfig(collection_name=collection_name)
            vector_store = VectorStore(
                embeddings=rag_pipeline.embeddings, config=config
            )
            vector_store.create_collection()
            rag_pipeline.vector_stores[collection_name] = vector_store

    return rag_pipeline


def validate_file(file: UploadFile) -> tuple[bool, str]:
    """Validate file type v√† size"""
    if not file.filename:
        return False, "Filename is required"

    ext = os.path.splitext(file.filename.lower())[1]
    if ext not in SUPPORTED_EXTENSIONS:
        return (
            False,
            f"Unsupported file type. Supported: {', '.join(SUPPORTED_EXTENSIONS)}",
        )

    return True, "Valid"


def format_file_info(
    file: UploadFile, size: int, storage_info: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Format file info cho response"""
    info = {
        "filename": file.filename,
        "file_size": size,
        "file_extension": (
            os.path.splitext(file.filename.lower())[1] if file.filename else ""
        ),
        "content_type": file.content_type or "application/octet-stream",
        "upload_time": datetime.now().isoformat(),
    }

    if storage_info:
        info.update(
            {
                "storage_path": storage_info["storage_path"],
                "storage_time": storage_info["storage_time"],
            }
        )

    return info


@router.post(
    "/knowledge-bases",
    response_model=KnowledgeBaseResponse,
    summary="üìö T·∫°o knowledge base m·ªõi",
    description="T·∫°o knowledge base m·ªõi trong vector store. M·ªói knowledge base l√† m·ªôt collection ri√™ng bi·ªát.",
)
async def create_knowledge_base(kb_data: KnowledgeBaseCreate):
    """
    T·∫°o knowledge base m·ªõi trong vector store.

    - **name**: T√™n c·ªßa knowledge base, s·∫Ω ƒë∆∞·ª£c d√πng l√†m collection name
    - **description**: M√¥ t·∫£ v·ªÅ knowledge base (optional)
    - **metadata**: Metadata b·ªï sung (optional)
    """
    try:
        # Kh·ªüi t·∫°o VectorStore v·ªõi collection name m·ªõi
        config = VectorStoreConfig(collection_name=kb_data.name)
        vector_store = VectorStore(config=config)
        success = vector_store.create_collection(force_recreate=True)

        if not success:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Kh√¥ng th·ªÉ t·∫°o collection trong vector store",
            )

        info = vector_store.get_collection_info()

        # ƒê·∫£m b·∫£o lu√¥n c√≥ created_at
        created_at = info.get("created_at")
        if not created_at:
            created_at = datetime.now().isoformat()

        return KnowledgeBaseResponse(
            name=kb_data.name,
            description=kb_data.description,
            metadata=kb_data.metadata,
            collection_info=info,
            created_at=created_at,
        )

    except Exception as e:
        logger.error(f"Error creating KB: {e}")
        raise HTTPException(500, detail=f"L·ªói khi t·∫°o knowledge base: {str(e)}")


@router.get(
    "/knowledge-bases",
    response_model=KnowledgeBaseList,
    summary="üìö L·∫•y danh s√°ch knowledge bases",
    description="L·∫•y danh s√°ch t·∫•t c·∫£ knowledge bases hi·ªán c√≥.",
)
async def list_knowledge_bases():
    """
    L·∫•y danh s√°ch t·∫•t c·∫£ knowledge bases.
    """
    try:
        # Kh·ªüi t·∫°o VectorStore kh√¥ng c·∫ßn embeddings v√¨ ch·ªâ d√πng ƒë·ªÉ qu·∫£n l√Ω collections
        vector_store = VectorStore()
        collections = vector_store.list_collections()

        kbs = []
        for collection in collections:
            # C·∫≠p nh·∫≠t collection name v√† l·∫•y th√¥ng tin
            vector_store.config.collection_name = collection.name
            info = vector_store.get_collection_info()

            # ƒê·∫£m b·∫£o lu√¥n c√≥ created_at
            created_at = info.get("created_at")
            if not created_at:
                created_at = datetime.now().isoformat()

            kbs.append(
                KnowledgeBaseResponse(
                    name=collection.name,
                    description=None,  # Collection info kh√¥ng c√≥ description
                    metadata=None,  # Collection info kh√¥ng c√≥ metadata
                    collection_info=info,
                    created_at=created_at,
                )
            )

        return KnowledgeBaseList(
            knowledge_bases=kbs,
            total=len(kbs),
        )

    except Exception as e:
        logger.error(f"Error listing KBs: {e}")
        raise HTTPException(
            500, detail=f"L·ªói khi l·∫•y danh s√°ch knowledge bases: {str(e)}"
        )


@router.delete(
    "/knowledge-bases/{name}",
    response_model=Dict[str, Any],
    summary="üóëÔ∏è X√≥a knowledge base",
    description="X√≥a knowledge base, t·∫•t c·∫£ documents v√† d·ªØ li·ªáu l∆∞u tr·ªØ trong MinIO.",
)
async def delete_knowledge_base(name: str):
    """
    X√≥a knowledge base.

    - **name**: T√™n c·ªßa knowledge base c·∫ßn x√≥a
    """
    try:
        # X√≥a collection trong vector store
        config = VectorStoreConfig(collection_name=name)
        vector_store = VectorStore(config=config)
        vector_store_success = vector_store.delete_collection()

        if not vector_store_success:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Kh√¥ng th·ªÉ x√≥a knowledge base {name} trong vector store",
            )

        # X√≥a folder v√† t·∫•t c·∫£ documents trong MinIO
        minio_success = document_storage.delete_collection_folder(name)
        if not minio_success:
            logger.warning(f"Kh√¥ng th·ªÉ x√≥a documents c·ªßa collection {name} trong MinIO")

        return {
            "success": True,
            "message": f"ƒê√£ x√≥a knowledge base {name} v√† t·∫•t c·∫£ d·ªØ li·ªáu li√™n quan",
            "time": datetime.now().isoformat(),
            "details": {
                "vector_store_deleted": vector_store_success,
                "minio_documents_deleted": minio_success,
            },
        }

    except Exception as e:
        logger.error(f"Error deleting KB: {e}")
        raise HTTPException(500, detail=f"L·ªói khi x√≥a knowledge base: {str(e)}")


@router.post(
    "/knowledge-bases/{name}/documents",
    response_model=FileUploadResponse,
    summary="üìÑ Upload document v√†o knowledge base",
    description="""Upload v√† process document v√†o knowledge base c·ª• th·ªÉ.""",
)
async def upload_document(
    name: str,
    file: UploadFile = File(...),
    chunk_size: int = Form(1000),
    chunk_overlap: int = Form(200),
    metadata: Dict[str, Any] = Form({}),
):
    """
    Upload v√† process document v√†o knowledge base.

    - **name**: T√™n c·ªßa knowledge base
    - **file**: File c·∫ßn upload
    - **chunk_size**: K√≠ch th∆∞·ªõc m·ªói chunk (default: 1000)
    - **chunk_overlap**: ƒê·ªô overlap gi·ªØa c√°c chunks (default: 200)
    - **metadata**: Metadata b·ªï sung cho document
    """
    start_time = time.time()
    temp_file = None

    try:
        valid, msg = validate_file(file)
        if not valid:
            raise HTTPException(400, detail=msg)

        content = await file.read()
        size = len(content)
        if size > MAX_FILE_SIZE:
            raise HTTPException(413, detail="File too large.")

        # Store file in MinIO with folder structure
        storage_info = document_storage.store_document(
            file_data=content,
            filename=file.filename or "unknown",
            knowledge_name=name,
            content_type=file.content_type or "application/octet-stream",
            metadata=metadata,
        )

        # Add storage info to metadata
        metadata.update(
            {
                "storage_path": storage_info["storage_path"],
                "storage_time": storage_info["storage_time"],
            }
        )

        with tempfile.NamedTemporaryFile(
            delete=False, suffix=os.path.splitext(file.filename or "")[1]
        ) as tmp:
            tmp.write(content)
            temp_file = tmp.name

        # Get pipeline v·ªõi collection name c·ª• th·ªÉ
        pipeline = get_rag_pipeline(name)

        # Update chunk config n·∫øu kh√°c default
        if chunk_size != 1000 or chunk_overlap != 200:
            config = RAGPipelineConfig(
                chunking_config=ChunkingConfig(
                    chunk_size=chunk_size, chunk_overlap=chunk_overlap
                ),
                embedding_config=EmbeddingConfig(),
                vector_store_config=VectorStoreConfig(collection_name=name),
            )
            global rag_pipeline
            rag_pipeline = RAGPipeline(config)
            pipeline = rag_pipeline

        # Add file info v√†o metadata
        metadata.update(
            {
                "uploaded_filename": file.filename,
                "upload_time": datetime.now().isoformat(),
                "file_size_bytes": size,
                "knowledge_base": name,
            }
        )

        doc_ids = pipeline.process_and_store(
            temp_file, collection_name=name, extra_metadata=metadata
        )
        stats = pipeline.get_stats()
        processing_time = round(time.time() - start_time, 2)

        logger.info(
            f"Uploaded {file.filename} to KB {name}: {len(doc_ids)} vectors in {processing_time}s"
        )

        return FileUploadResponse(
            success=True,
            message=f"Processed {file.filename} -> {len(doc_ids)} vectors",
            file_info=FileInfoModel(**format_file_info(file, size, storage_info)),
            document_ids=doc_ids,
            statistics=stats,
            processing_time=processing_time,
        )

    finally:
        if temp_file and os.path.exists(temp_file):
            os.unlink(temp_file)


@router.get(
    "/knowledge-bases/{name}/stats",
    response_model=Dict[str, Any],
    summary="üìä Knowledge Base Stats",
    description="L·∫•y th·ªëng k√™ c·ªßa knowledge base.",
)
async def get_knowledge_base_stats(name: str):
    """
    L·∫•y th·ªëng k√™ c·ªßa knowledge base.

    - **name**: T√™n c·ªßa knowledge base
    """
    try:
        pipeline = get_rag_pipeline(name)
        stats = pipeline.get_stats()
        info = pipeline.vector_stores[name].get_collection_info()

        return {
            "success": True,
            "name": name,
            "stats": stats,
            "collection_info": info,
            "time": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error getting KB stats: {e}")
        raise HTTPException(
            500, detail=f"L·ªói khi l·∫•y th·ªëng k√™ knowledge base: {str(e)}"
        )


@router.get(
    "/knowledge-bases/{name}/documents/stats",
    response_model=CollectionStats,
    summary="üìä Th·ªëng k√™ documents trong knowledge base",
    description="L·∫•y th√¥ng tin th·ªëng k√™ v·ªÅ c√°c documents trong m·ªôt knowledge base.",
)
async def get_documents_stats(name: str):
    """
    L·∫•y th·ªëng k√™ v·ªÅ documents trong knowledge base.

    - **name**: T√™n c·ªßa knowledge base c·∫ßn th·ªëng k√™
    """
    try:
        # Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng
        vector_store = VectorStore()
        collections = vector_store.list_collections()
        if not any(c.name == name for c in collections):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Kh√¥ng t√¨m th·∫•y knowledge base {name}",
            )

        # L·∫•y th·ªëng k√™ t·ª´ MinIO
        stats = document_storage.get_collection_stats(name)

        # Format dung l∆∞·ª£ng ƒë·ªÉ d·ªÖ ƒë·ªçc
        total_size_mb = round(stats["total_size_bytes"] / (1024 * 1024), 2)
        logger.info(
            f"Collection {name} stats: {stats['total_documents']} documents, {total_size_mb} MB"
        )

        return CollectionStats(**stats)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting documents stats: {e}")
        raise HTTPException(500, detail=f"L·ªói khi l·∫•y th·ªëng k√™ documents: {str(e)}")


@router.post(
    "/search",
    response_model=MultiCollectionSearchResponse,
    summary="üîç T√¨m ki·∫øm tr√™n nhi·ªÅu knowledge bases",
    description="Th·ª±c hi·ªán t√¨m ki·∫øm hybrid (BM25 + Vector) tr√™n nhi·ªÅu knowledge bases.",
)
async def multi_collection_search(request: MultiCollectionSearchRequest):
    """
    T√¨m ki·∫øm tr√™n nhi·ªÅu knowledge bases.

    - **query**: C√¢u h·ªèi/query c·∫ßn t√¨m ki·∫øm
    - **collection_names**: Danh s√°ch c√°c knowledge bases c·∫ßn t√¨m ki·∫øm
    - **top_k**: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ (default: 5)
    - **score_threshold**: Ng∆∞·ª°ng ƒëi·ªÉm t·ªëi thi·ªÉu (default: 0.3)
    """
    try:
        start_time = time.time()

        # Ki·ªÉm tra c√°c collections c√≥ t·ªìn t·∫°i kh√¥ng
        vector_store = VectorStore()
        collections = vector_store.list_collections()
        existing_collections = {c.name for c in collections}

        invalid_collections = set(request.collection_names) - existing_collections
        if invalid_collections:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Kh√¥ng t√¨m th·∫•y knowledge bases: {', '.join(invalid_collections)}",
            )

        # Kh·ªüi t·∫°o embedding model
        try:
            embeddings = Embedding()
            logger.info("Successfully initialized embedding model")
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"L·ªói kh·ªüi t·∫°o embedding model: {str(e)}",
            )

        # T·∫°o vector stores cho t·ª´ng collection
        vector_stores = {}
        for name in request.collection_names:
            try:
                config = VectorStoreConfig(collection_name=name)
                store = VectorStore(embeddings=embeddings, config=config)
                store.create_collection()  # K·∫øt n·ªëi t·ªõi collection ƒë√£ t·ªìn t·∫°i
                vector_stores[name] = store
                logger.info(f"Successfully created vector store for collection: {name}")
            except Exception as e:
                logger.error(
                    f"Failed to create vector store for collection {name}: {e}"
                )
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"L·ªói t·∫°o vector store cho collection {name}: {str(e)}",
                )

        # Validate vector stores were created properly
        if not vector_stores:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Kh√¥ng th·ªÉ t·∫°o ƒë∆∞·ª£c vector stores n√†o",
            )

        # T·∫°o retriever cho multi-collection search
        try:
            hybrid_config = HybridSearchConfig(
                bm25_weight=0.3,
                vector_weight=0.7,
                top_k=request.top_k or 5,
                score_threshold=(
                    request.score_threshold
                    if request.score_threshold is not None
                    else 0.3
                ),
            )

            multi_config = MultiCollectionConfig(
                normalize_scores=True,
                top_k=request.top_k or 5,
                score_threshold=(
                    request.score_threshold
                    if request.score_threshold is not None
                    else 0.3
                ),
            )

            logger.info(
                f"Creating MultiCollectionHybridRetriever with {len(vector_stores)} collections"
            )
            retriever = MultiCollectionHybridRetriever(
                vector_stores=vector_stores,
                hybrid_config=hybrid_config,
                multi_collection_config=multi_config,
            )

            # Validate retriever was created properly using utility function
            is_valid, error_msg = validate_retriever_initialization(retriever)
            if not is_valid:
                raise ValueError(f"Retriever validation failed: {error_msg}")

            # Log retriever type information for debugging
            retriever_info = check_retriever_type(retriever)
            logger.info(f"Successfully created retriever: {retriever_info}")

        except Exception as e:
            logger.error(f"Failed to create retriever: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"L·ªói t·∫°o retriever: {str(e)}",
            )

        # Th·ª±c hi·ªán t√¨m ki·∫øm
        try:
            logger.info(f"Starting search with query: {request.query[:50]}...")
            results = retriever.get_relevant_documents(request.query)
            logger.info(f"Search completed, found {len(results)} results")
        except Exception as e:
            logger.error(f"Search failed: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"L·ªói th·ª±c hi·ªán t√¨m ki·∫øm: {str(e)}",
            )

        # Format k·∫øt qu·∫£
        search_results = []
        for doc in results:
            search_results.append(
                SearchResult(
                    content=doc.page_content,
                    metadata=doc.metadata,
                    score=float(doc.metadata.get("hybrid_score", 0.0)),
                    collection_name=doc.metadata.get("collection_name", "unknown"),
                )
            )

        # L·∫•y th·ªëng k√™ theo collection
        collection_stats = {}
        for name in request.collection_names:
            try:
                stats = document_storage.get_collection_stats(name)
                collection_stats[name] = stats
            except Exception as e:
                logger.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c th·ªëng k√™ cho {name}: {e}")
                collection_stats[name] = {"error": str(e)}

        processing_time = time.time() - start_time

        return MultiCollectionSearchResponse(
            results=search_results,
            total_results=len(search_results),
            processing_time=processing_time,
            collection_stats=collection_stats,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during multi-collection search: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm: {str(e)}",
        )
