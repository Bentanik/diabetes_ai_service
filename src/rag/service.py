"""
Dá»‹ch vá»¥ RAG nÃ¢ng cao tÃ­ch há»£p RAGFlow vÃ  HuggingFace.
Cung cáº¥p kháº£ nÄƒng xá»­ lÃ½ tÃ i liá»‡u vÃ  truy váº¥n tiáº¿ng Viá»‡t tá»‘i Æ°u.
"""

import os
from typing import List, Dict, Any, Optional
from langchain.schema import Document

from .embeddings import get_embedding_service, MultiEmbeddingService, Embeddings
from .chunker import get_vietnamese_chunker
from .hybrid_retriever import get_hybrid_retriever
from core.llm_client import get_llm
from langchain_qdrant import QdrantVectorStore
from core.logging_config import get_logger

logger = get_logger(__name__)


class RAGService:
    """
    Dá»‹ch vá»¥ RAG nÃ¢ng cao vá»›i:
    - HuggingFace embeddings tá»‘i Æ°u cho tiáº¿ng Viá»‡t
    - RAGFlow PDF parsing cáº£i tiáº¿n
    - Chunking thÃ´ng minh vá»›i báº£o toÃ n cáº¥u trÃºc
    - Prompt engineering nÃ¢ng cao
    """

    def __init__(
        self,
        # Cáº¥u hÃ¬nh embedding
        embedding_provider: str = "huggingface",
        embedding_model: str = "intfloat/multilingual-e5-base",
        embedding_api_key: Optional[str] = None,
        embedding_device: str = "auto",
        # Cáº¥u hÃ¬nh Qdrant vector store
        collection_name: str = "vietnamese_knowledge_base",
        qdrant_url: str = "http://localhost:6333",
        qdrant_api_key: Optional[str] = None,
        # Cáº¥u hÃ¬nh chunking
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        use_ragflow_pdf: bool = True,
        preserve_structure: bool = True,
        # Cáº¥u hÃ¬nh retrieval
        retrieval_k: int = 5,
        score_threshold: float = 0.1,
        use_reranking: bool = False,
    ):
        """
        Khá»Ÿi táº¡o dá»‹ch vá»¥ RAG nÃ¢ng cao.

        Args:
            embedding_provider: Loáº¡i provider (huggingface hoáº·c openai)
            embedding_model: TÃªn model embedding
            embedding_api_key: API key náº¿u cáº§n
            embedding_device: Thiáº¿t bá»‹ cháº¡y (cpu/cuda)
            collection_name: TÃªn collection vector store
            vectorstore_dir: ThÆ° má»¥c lÆ°u vector store
            chunk_size: KÃ­ch thÆ°á»›c chunk tá»‘i Ä‘a
            chunk_overlap: Äá»™ chá»“ng láº¥p chunks
            use_ragflow_pdf: Sá»­ dá»¥ng RAGFlow PDF parser
            preserve_structure: Báº£o toÃ n cáº¥u trÃºc tÃ i liá»‡u
            retrieval_k: Sá»‘ documents truy váº¥n
            score_threshold: NgÆ°á»¡ng Ä‘á»™ tÆ°Æ¡ng tá»± tá»‘i thiá»ƒu
            use_reranking: Sá»­ dá»¥ng reranking
        """

        # LÆ°u cáº¥u hÃ¬nh
        self.config = {
            "embedding": {
                "provider": embedding_provider,
                "model": embedding_model,
                "device": embedding_device,
            },
            "chunking": {
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "use_ragflow_pdf": use_ragflow_pdf,
                "preserve_structure": preserve_structure,
            },
            "retrieval": {
                "k": retrieval_k,
                "score_threshold": score_threshold,
                "use_reranking": use_reranking,
            },
        }

        # Khá»Ÿi táº¡o cÃ¡c components
        logger.info("Äang khá»Ÿi táº¡o cÃ¡c components RAG nÃ¢ng cao...")

        # 1. Dá»‹ch vá»¥ embedding
        self.embedding_service = get_embedding_service(
            provider=embedding_provider,
            model_name=embedding_model,
            api_key=embedding_api_key,
            device=embedding_device,
        )

        # 2. Bá»™ chia nhá» tÃ i liá»‡u tiáº¿ng Viá»‡t
        self.chunker = get_vietnamese_chunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            use_ragflow_pdf=use_ragflow_pdf,
        )

        # 3. Qdrant Vector Store (langchain-qdrant official)
        # Láº¥y LangChain embeddings object
        if isinstance(self.embedding_service.embeddings, Embeddings):
            langchain_embeddings = self.embedding_service.embeddings.embeddings
        else:
            langchain_embeddings = self.embedding_service.embeddings

        self.vectorstore = QdrantVectorStore.from_existing_collection(
            embedding=langchain_embeddings,
            collection_name=collection_name,
            url=qdrant_url,
            api_key=qdrant_api_key,
        )

        logger.info(f"Using Qdrant vectorstore: {qdrant_url}")
        logger.info(f"Collection: {collection_name}")

        # 4. Hybrid Retriever (BM25 + Embedding) - MAIN RETRIEVER
        self.hybrid_retriever = get_hybrid_retriever(
            vectorstore=self.vectorstore,
            default_k=retrieval_k,
            score_threshold=score_threshold,
        )

        # Note: Regular retriever Ä‘Ã£ Ä‘Æ°á»£c thay tháº¿ bá»Ÿi hybrid_retriever
        # Hybrid cÃ³ thá»ƒ lÃ m táº¥t cáº£: hybrid, bm25_only, embedding_only

        # 5. LLM client
        self.llm = get_llm()

        # LÆ°u tÃ¹y chá»n xá»­ lÃ½
        self.preserve_structure = preserve_structure
        self.use_reranking = use_reranking

        logger.info("ÄÃ£ khá»Ÿi táº¡o dá»‹ch vá»¥ RAG nÃ¢ng cao thÃ nh cÃ´ng")
        logger.info(f"Embedding: {embedding_provider} ({embedding_model})")
        logger.info(
            f"Chunking: ragflow_pdf={use_ragflow_pdf}, structure={preserve_structure}"
        )
        logger.info("Hybrid Retrieval: BM25 + Embedding Ä‘Æ°á»£c kÃ­ch hoáº¡t")

    async def add_documents_from_files(
        self, file_paths: List[str], preserve_structure: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        ThÃªm documents tá»« files vá»›i xá»­ lÃ½ nÃ¢ng cao.

        Args:
            file_paths: Danh sÃ¡ch Ä‘Æ°á»ng dáº«n files
            preserve_structure: Ghi Ä‘Ã¨ cáº¥u hÃ¬nh báº£o toÃ n cáº¥u trÃºc

        Returns:
            Káº¿t quáº£ xá»­ lÃ½ chi tiáº¿t
        """
        try:
            # Sá»­ dá»¥ng config máº·c Ä‘á»‹nh náº¿u khÃ´ng chá»‰ Ä‘á»‹nh
            if preserve_structure is None:
                preserve_structure = self.preserve_structure

            # Xá»­ lÃ½ tÃ i liá»‡u nÃ¢ng cao
            logger.info(
                f"Äang xá»­ lÃ½ {len(file_paths)} files vá»›i bá»™ chunker tiáº¿ng Viá»‡t..."
            )

            chunks = self.chunker.process_multiple_files(
                file_paths=file_paths, preserve_structure=preserve_structure
            )

            if not chunks:
                return {
                    "success": False,
                    "message": "KhÃ´ng cÃ³ documents nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng",
                    "files_processed": 0,
                    "chunks_added": 0,
                    "processing_details": [],
                }

            # ThÃªm vÃ o vector store
            logger.info(f"Äang thÃªm {len(chunks)} chunks vÃ o vector store...")
            document_ids = self.vectorstore.add_documents(chunks)

            # Initialize BM25 vá»›i documents má»›i
            if self.hybrid_retriever.bm25_retriever is None:
                logger.info("Khá»Ÿi táº¡o BM25 retriever vá»›i documents má»›i...")
                self.hybrid_retriever.add_documents_to_bm25(chunks)
            else:
                # ThÃªm documents má»›i vÃ o BM25 existing
                self.hybrid_retriever.add_documents_to_bm25(chunks)

            # Táº¡o chi tiáº¿t xá»­ lÃ½
            processing_details = self._generate_processing_details(file_paths, chunks)

            logger.info(
                f"ÄÃ£ thÃªm thÃ nh cÃ´ng {len(chunks)} chunks tá»« {len(file_paths)} files"
            )

            return {
                "success": True,
                "message": f"ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng {len(file_paths)} files â†’ {len(chunks)} chunks",
                "files_processed": len(file_paths),
                "chunks_added": len(chunks),
                "document_ids": document_ids,
                "processing_details": processing_details,
                "advanced_features": {
                    "ragflow_pdf_parsing": self.chunker.use_ragflow_pdf,
                    "structure_preservation": preserve_structure,
                    "vietnamese_optimization": True,
                    "embedding_provider": self.config["embedding"]["provider"],
                },
            }

        except Exception as e:
            logger.error(f"Lá»—i trong xá»­ lÃ½ documents nÃ¢ng cao: {e}")
            return {
                "success": False,
                "message": f"Lá»—i xá»­ lÃ½ documents: {str(e)}",
                "files_processed": 0,
                "chunks_added": 0,
                "processing_details": [],
            }

    async def add_text(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
        preserve_structure: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """ThÃªm raw text vá»›i xá»­ lÃ½ nÃ¢ng cao."""
        try:
            if preserve_structure is None:
                preserve_structure = self.preserve_structure

            # Xá»­ lÃ½ text nÃ¢ng cao
            chunks = self.chunker.process_text(
                text=text, metadata=metadata, preserve_structure=preserve_structure
            )

            # ThÃªm vÃ o vector store
            document_ids = self.vectorstore.add_documents(chunks)

            # Initialize BM25 vá»›i documents má»›i
            if self.hybrid_retriever.bm25_retriever is None:
                logger.info("Khá»Ÿi táº¡o BM25 retriever vá»›i documents má»›i...")
                self.hybrid_retriever.add_documents_to_bm25(chunks)
            else:
                # ThÃªm documents má»›i vÃ o BM25 existing
                self.hybrid_retriever.add_documents_to_bm25(chunks)

            logger.info(f"ÄÃ£ thÃªm text vá»›i {len(chunks)} chunks (tá»‘i Æ°u tiáº¿ng Viá»‡t)")

            return {
                "success": True,
                "message": f"ÄÃ£ thÃªm text vá»›i {len(chunks)} chunks (nÃ¢ng cao)",
                "chunks_added": len(chunks),
                "document_ids": document_ids,
                "advanced_features": {
                    "structure_preservation": preserve_structure,
                    "vietnamese_optimization": True,
                    "embedding_provider": self.config["embedding"]["provider"],
                },
            }

        except Exception as e:
            logger.error(f"Lá»—i thÃªm text: {e}")
            return {
                "success": False,
                "message": f"Lá»—i thÃªm text: {str(e)}",
                "chunks_added": 0,
            }

    async def query(
        self,
        question: str,
        k: Optional[int] = None,
        use_reranking: Optional[bool] = None,
        include_sources: bool = True,
        vietnamese_prompt: bool = True,
    ) -> Dict[str, Any]:
        """
        Truy váº¥n nÃ¢ng cao vá»›i cáº£i thiá»‡n cho tiáº¿ng Viá»‡t.

        Args:
            question: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
            k: Sá»‘ documents truy váº¥n
            use_reranking: Sá»­ dá»¥ng reranking (ghi Ä‘Ã¨ config)
            include_sources: Bao gá»“m thÃ´ng tin nguá»“n
            vietnamese_prompt: Sá»­ dá»¥ng prompt tá»‘i Æ°u tiáº¿ng Viá»‡t

        Returns:
            Káº¿t quáº£ truy váº¥n nÃ¢ng cao
        """
        try:
            # Sá»­ dá»¥ng config máº·c Ä‘á»‹nh náº¿u khÃ´ng chá»‰ Ä‘á»‹nh
            if k is None:
                k = self.config["retrieval"]["k"]
            if use_reranking is None:
                use_reranking = self.use_reranking

            logger.info(f"Äang xá»­ lÃ½ truy váº¥n nÃ¢ng cao: {question[:100]}...")

            # Retrieval nÃ¢ng cao
            # Sá»­ dá»¥ng hybrid retriever vá»›i embedding_only mode Ä‘á»ƒ maintain compatibility
            if use_reranking:
                # Äáº£m báº£o k khÃ´ng None
                k_value = k if k is not None else self.config["retrieval"]["k"]
                # DÃ¹ng hybrid vá»›i embedding_only + manual reranking logic
                candidates = await self.hybrid_retriever.hybrid_search(
                    query=question,
                    k=k_value * 2,  # Láº¥y nhiá»u Ä‘á»ƒ rerank
                    method="embedding_only",
                    include_scores=True,
                )
                # Simple reranking based on scores
                candidates.sort(
                    key=lambda doc: doc.metadata.get("similarity_score", 0),
                    reverse=True,
                )
                relevant_docs = candidates[:k_value]
            else:
                relevant_docs = await self.hybrid_retriever.hybrid_search(
                    query=question, k=k, method="embedding_only", include_scores=True
                )

            if not relevant_docs:
                return {
                    "success": True,
                    "answer": "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.",
                    "sources": [],
                    "num_sources": 0,
                    "retrieval_method": "advanced",
                    "confidence_score": 0.0,
                }

            # XÃ¢y dá»±ng context nÃ¢ng cao
            context = self._build_vietnamese_context(relevant_docs)

            # Táº¡o prompt nÃ¢ng cao
            if vietnamese_prompt:
                prompt = self._build_vietnamese_prompt(question, context, relevant_docs)
            else:
                prompt = self._build_basic_prompt(question, context)

            # Táº¡o cÃ¢u tráº£ lá»i
            answer = await self.llm.generate(prompt)

            # TÃ­nh Ä‘iá»ƒm tin cáº­y
            confidence_score = self._calculate_confidence_score(relevant_docs)

            # Chuáº©n bá»‹ thÃ´ng tin nguá»“n nÃ¢ng cao
            sources = []
            if include_sources:
                sources = self._extract_vietnamese_sources(relevant_docs)

            logger.info(
                f"Truy váº¥n nÃ¢ng cao hoÃ n thÃ nh vá»›i {len(relevant_docs)} sources"
            )

            return {
                "success": True,
                "answer": answer.strip(),
                "sources": sources,
                "num_sources": len(relevant_docs),
                "retrieval_method": (
                    "advanced_reranking" if use_reranking else "advanced"
                ),
                "confidence_score": confidence_score,
                "processing_info": {
                    "embedding_provider": self.config["embedding"]["provider"],
                    "reranking_used": use_reranking,
                    "vietnamese_prompt": vietnamese_prompt,
                    "vietnamese_optimized": True,
                },
            }

        except Exception as e:
            logger.error(f"Lá»—i trong truy váº¥n nÃ¢ng cao: {e}")
            return {
                "success": False,
                "answer": f"Lá»—i xá»­ lÃ½ cÃ¢u há»i: {str(e)}",
                "sources": [],
                "num_sources": 0,
                "confidence_score": 0.0,
            }

    async def hybrid_query(
        self,
        question: str,
        k: Optional[int] = None,
        method: str = "hybrid",  # "hybrid", "bm25_only", "embedding_only"
        include_sources: bool = True,
        vietnamese_prompt: bool = True,
    ) -> Dict[str, Any]:
        """
        Truy váº¥n hybrid sá»­ dá»¥ng cáº£ BM25 vÃ  embedding search.

        Args:
            question: CÃ¢u há»i cáº§n tráº£ lá»i
            k: Sá»‘ documents truy váº¥n
            method: PhÆ°Æ¡ng phÃ¡p ("hybrid", "bm25_only", "embedding_only")
            include_sources: Bao gá»“m thÃ´ng tin nguá»“n
            vietnamese_prompt: Sá»­ dá»¥ng prompt tiáº¿ng Viá»‡t

        Returns:
            Káº¿t quáº£ truy váº¥n vá»›i thÃ´ng tin vá» phÆ°Æ¡ng phÃ¡p Ä‘Ã£ sá»­ dá»¥ng
        """
        try:
            k = k or self.config["retrieval"]["k"]

            # Hybrid retrieval
            relevant_docs = await self.hybrid_retriever.hybrid_search(
                query=question, k=k, method=method, include_scores=True
            )

            if not relevant_docs:
                return {
                    "success": True,
                    "answer": "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong cÆ¡ sá»Ÿ tri thá»©c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.",
                    "sources": [],
                    "num_sources": 0,
                    "retrieval_method": f"hybrid_{method}",
                    "confidence_score": 0.0,
                    "hybrid_info": {
                        "method_used": method,
                        "bm25_available": self.hybrid_retriever.bm25_retriever
                        is not None,
                        "documents_searched": 0,
                    },
                }

            # XÃ¢y dá»±ng context
            context = self._build_vietnamese_context(relevant_docs)

            # Táº¡o prompt
            if vietnamese_prompt:
                prompt = self._build_vietnamese_hybrid_prompt(
                    question, context, relevant_docs, method
                )
            else:
                prompt = self._build_basic_prompt(question, context)

            # Gá»i LLM
            answer = await self.llm.generate(prompt)

            # TÃ­nh confidence score
            confidence_score = self._calculate_confidence_score(relevant_docs)

            # Chuáº©n bá»‹ sources
            sources = []
            if include_sources:
                sources = self._extract_vietnamese_sources(relevant_docs)

            # Thá»‘ng kÃª hybrid
            hybrid_stats = self._extract_hybrid_stats(relevant_docs, method)

            return {
                "success": True,
                "answer": answer.strip(),
                "sources": sources,
                "num_sources": len(relevant_docs),
                "retrieval_method": f"hybrid_{method}",
                "confidence_score": confidence_score,
                "hybrid_info": {
                    "method_used": method,
                    "bm25_available": self.hybrid_retriever.bm25_retriever is not None,
                    "bm25_weight": hybrid_stats.get("bm25_weight", 0),
                    "embedding_weight": hybrid_stats.get("embedding_weight", 0),
                    "avg_bm25_score": hybrid_stats.get("avg_bm25_score", 0),
                    "avg_embedding_score": hybrid_stats.get("avg_embedding_score", 0),
                    "documents_searched": len(relevant_docs),
                },
                "processing_info": {
                    "embedding_provider": self.config["embedding"]["provider"],
                    "vietnamese_prompt": vietnamese_prompt,
                    "vietnamese_optimized": True,
                    "hybrid_fusion": method == "hybrid",
                },
            }

        except Exception as e:
            logger.error(f"Lá»—i trong hybrid query: {e}")
            return {
                "success": False,
                "answer": f"Lá»—i xá»­ lÃ½ cÃ¢u há»i hybrid: {str(e)}",
                "sources": [],
                "num_sources": 0,
                "confidence_score": 0.0,
                "hybrid_info": {"method_used": method, "error": str(e)},
            }

    async def compare_retrieval_methods(
        self, question: str, k: int = 5
    ) -> Dict[str, Any]:
        """
        So sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p retrieval khÃ¡c nhau.

        Args:
            question: CÃ¢u há»i Ä‘á»ƒ test
            k: Sá»‘ documents Ä‘á»ƒ so sÃ¡nh

        Returns:
            So sÃ¡nh chi tiáº¿t giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p
        """
        try:
            # So sÃ¡nh hybrid methods
            hybrid_comparison = await self.hybrid_retriever.compare_methods(question, k)

            # ThÃªm regular embedding-only retrieval Ä‘á»ƒ so sÃ¡nh
            regular_docs = await self.hybrid_retriever.hybrid_search(
                query=question, k=k, method="embedding_only", include_scores=True
            )

            regular_results = {
                "count": len(regular_docs),
                "avg_score": (
                    sum(doc.metadata.get("similarity_score", 0) for doc in regular_docs)
                    / len(regular_docs)
                    if regular_docs
                    else 0
                ),
                "documents": [
                    {
                        "content_preview": doc.page_content[:100] + "...",
                        "score": doc.metadata.get("similarity_score", 0),
                        "source": doc.metadata.get("source_file", "unknown"),
                    }
                    for doc in regular_docs[:3]
                ],
            }

            hybrid_comparison["methods"]["regular_embedding"] = regular_results

            # ThÃªm metadata
            hybrid_comparison["comparison_info"] = {
                "question": question,
                "k": k,
                "timestamp": "N/A",  # Could add timestamp
                "recommendation": self._recommend_best_method(
                    hybrid_comparison["methods"]
                ),
            }

            return hybrid_comparison

        except Exception as e:
            logger.error(f"Lá»—i so sÃ¡nh retrieval methods: {e}")
            return {"error": str(e), "question": question, "k": k}

    def _build_vietnamese_hybrid_prompt(
        self, question: str, context: str, documents: List[Document], method: str
    ) -> str:
        """XÃ¢y dá»±ng prompt hybrid vá»›i thÃ´ng tin vá» phÆ°Æ¡ng phÃ¡p retrieval."""
        doc_info = self._analyze_vietnamese_documents(documents)
        hybrid_info = self._extract_hybrid_stats(documents, method)

        method_description = {
            "hybrid": "Káº¿t há»£p BM25 (tá»« khÃ³a) + Embedding (ngá»¯ nghÄ©a)",
            "bm25_only": "BM25 - TÃ¬m kiáº¿m theo tá»« khÃ³a chÃ­nh xÃ¡c",
            "embedding_only": "Embedding - TÃ¬m kiáº¿m theo ngá»¯ nghÄ©a",
        }

        prompt = f"""Báº¡n lÃ  má»™t AI assistant thÃ´ng minh sá»­ dá»¥ng há»‡ thá»‘ng tÃ¬m kiáº¿m hybrid tiÃªn tiáº¿n. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c tÃ¬m kiáº¿m báº±ng phÆ°Æ¡ng phÃ¡p {method_description.get(method, method)}.

ðŸ” **THÃ”NG TIN TÃŒM KIáº¾M:**
- PhÆ°Æ¡ng phÃ¡p: {method_description.get(method, method)}
- Sá»‘ nguá»“n tÃ¬m tháº¥y: {len(documents)}
- Äá»™ tin cáº­y trung bÃ¬nh: {doc_info['avg_confidence']:.3f}
{f"- Trá»ng sá»‘ BM25: {hybrid_info.get('bm25_weight', 0):.2f}" if method == "hybrid" else ""}
{f"- Trá»ng sá»‘ Embedding: {hybrid_info.get('embedding_weight', 0):.2f}" if method == "hybrid" else ""}

ðŸ“– **THÃ”NG TIN THAM KHáº¢O:**
{context}

â“ **CÃ‚U Há»ŽI:** {question}

ðŸ’¡ **HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:**
1. Sá»­ dá»¥ng thÃ´ng tin tá»« káº¿t quáº£ tÃ¬m kiáº¿m {method_description.get(method, method)}
2. TrÃ­ch dáº«n nguá»“n cá»¥ thá»ƒ khi cáº§n thiáº¿t
3. ÄÃ¡nh giÃ¡ Ä‘á»™ tin cáº­y cá»§a thÃ´ng tin dá»±a trÃªn phÆ°Æ¡ng phÃ¡p tÃ¬m kiáº¿m
4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c
5. NÃªu rÃµ náº¿u thÃ´ng tin khÃ´ng Ä‘á»§ hoáº·c cáº§n tÃ¬m hiá»ƒu thÃªm

**TRáº¢ Lá»œI:**"""

        return prompt

    def _extract_hybrid_stats(
        self, documents: List[Document], method: str
    ) -> Dict[str, Any]:
        """TrÃ­ch xuáº¥t thá»‘ng kÃª tá»« káº¿t quáº£ hybrid retrieval."""
        if not documents:
            return {}

        stats = {}
        bm25_scores = []
        embedding_scores = []

        for doc in documents:
            bm25_score = doc.metadata.get("bm25_score", 0)
            embedding_score = doc.metadata.get("embedding_score", 0)

            if bm25_score > 0:
                bm25_scores.append(bm25_score)
            if embedding_score > 0:
                embedding_scores.append(embedding_score)

            # Láº¥y weights tá»« document Ä‘áº§u tiÃªn (náº¿u cÃ³)
            if "bm25_weight" in doc.metadata:
                stats["bm25_weight"] = doc.metadata["bm25_weight"]
            if "embedding_weight" in doc.metadata:
                stats["embedding_weight"] = doc.metadata["embedding_weight"]

        if bm25_scores:
            stats["avg_bm25_score"] = sum(bm25_scores) / len(bm25_scores)
        if embedding_scores:
            stats["avg_embedding_score"] = sum(embedding_scores) / len(embedding_scores)

        return stats

    def _recommend_best_method(self, methods_results: Dict[str, Any]) -> str:
        """Äá» xuáº¥t phÆ°Æ¡ng phÃ¡p tá»‘t nháº¥t dá»±a trÃªn káº¿t quáº£."""
        if not methods_results:
            return "KhÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ Ä‘á» xuáº¥t"

        scores = {}
        for method, result in methods_results.items():
            if isinstance(result, dict) and "avg_score" in result:
                scores[method] = result["avg_score"]

        if not scores:
            return "KhÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡"

        best_method = max(scores.keys(), key=lambda x: scores[x])
        best_score = scores[best_method]

        recommendations = {
            "hybrid_fusion": "Hybrid fusion - CÃ¢n báº±ng tá»‘t nháº¥t giá»¯a tá»« khÃ³a vÃ  ngá»¯ nghÄ©a",
            "bm25_only": "BM25 - Tá»‘t cho tÃ¬m kiáº¿m tá»« khÃ³a chÃ­nh xÃ¡c vÃ  thuáº­t ngá»¯ ká»¹ thuáº­t",
            "embedding_only": "Embedding - Tá»‘t cho tÃ¬m kiáº¿m ngá»¯ nghÄ©a vÃ  tá»« Ä‘á»“ng nghÄ©a",
            "regular_embedding": "Embedding thÃ´ng thÆ°á»ng - ÄÆ¡n giáº£n vÃ  á»•n Ä‘á»‹nh",
        }

        return (
            f"{recommendations.get(best_method, best_method)} (Äiá»ƒm: {best_score:.3f})"
        )

    def _generate_processing_details(
        self, file_paths: List[str], chunks: List[Document]
    ) -> List[Dict]:
        """Táº¡o thÃ´ng tin chi tiáº¿t vá» quÃ¡ trÃ¬nh xá»­ lÃ½."""
        details = []

        # NhÃ³m chunks theo file nguá»“n
        file_chunks = {}
        for chunk in chunks:
            source_file = chunk.metadata.get("source_file", "unknown")
            if source_file not in file_chunks:
                file_chunks[source_file] = []
            file_chunks[source_file].append(chunk)

        for file_path in file_paths:
            filename = os.path.basename(file_path)
            file_chunks_list = file_chunks.get(filename, [])

            details.append(
                {
                    "file": filename,
                    "file_path": file_path,
                    "chunks_created": len(file_chunks_list),
                    "extraction_method": (
                        file_chunks_list[0].metadata.get(
                            "extraction_method", "standard"
                        )
                        if file_chunks_list
                        else "failed"
                    ),
                    "chunk_method": (
                        file_chunks_list[0].metadata.get("chunk_method", "standard")
                        if file_chunks_list
                        else "failed"
                    ),
                    "vietnamese_optimized": (
                        file_chunks_list[0].metadata.get("vietnamese_optimized", False)
                        if file_chunks_list
                        else False
                    ),
                }
            )

        return details

    def _build_vietnamese_context(self, documents: List[Document]) -> str:
        """XÃ¢y dá»±ng context nÃ¢ng cao vá»›i Ä‘á»‹nh dáº¡ng tiáº¿ng Viá»‡t."""
        if not documents:
            return ""

        context_parts = []

        for i, doc in enumerate(documents, 1):
            content = doc.page_content.strip()
            source = doc.metadata.get("source_file", "KhÃ´ng rÃµ")
            page = doc.metadata.get("page", "N/A")
            score = doc.metadata.get("similarity_score", 0.0)
            extraction_method = doc.metadata.get("extraction_method", "standard")

            # Äá»‹nh dáº¡ng context tiáº¿ng Viá»‡t
            context_parts.append(
                f"ðŸ“„ **Nguá»“n {i}:** {source} (Trang {page}) | Äá»™ liÃªn quan: {score:.3f} | PhÆ°Æ¡ng phÃ¡p: {extraction_method}\n"
                f"{content}\n"
                f"{'â”€' * 50}"
            )

        return "\n\n".join(context_parts)

    def _build_vietnamese_prompt(
        self, question: str, context: str, documents: List[Document]
    ) -> str:
        """XÃ¢y dá»±ng prompt nÃ¢ng cao tá»‘i Æ°u cho tiáº¿ng Viá»‡t."""
        # PhÃ¢n tÃ­ch documents
        doc_info = self._analyze_vietnamese_documents(documents)

        prompt = f"""Báº¡n lÃ  má»™t AI assistant chuyÃªn nghiá»‡p vÃ  thÃ´ng minh, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ há»— trá»£ ngÆ°á»i dÃ¹ng tiáº¿ng Viá»‡t. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.

ðŸ“‹ **THÃ”NG TIN TÃ€I LIá»†U:**
- Sá»‘ lÆ°á»£ng nguá»“n: {len(documents)}
- Loáº¡i tÃ i liá»‡u: {doc_info['document_types']}
- PhÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t: {doc_info['extraction_methods']}
- Äá»™ tin cáº­y trung bÃ¬nh: {doc_info['avg_confidence']:.3f}
- Tá»‘i Æ°u hÃ³a tiáº¿ng Viá»‡t: {doc_info['vietnamese_optimized']}

ðŸ“– **THÃ”NG TIN THAM KHáº¢O:**
{context}

â“ **CÃ‚U Há»ŽI:** {question}

ðŸ’¡ **HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:**
1. Dá»±a vÃ o thÃ´ng tin trÃªn Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c vÃ  chi tiáº¿t
2. TrÃ­ch dáº«n cá»¥ thá»ƒ nguá»“n tÃ i liá»‡u khi cáº§n thiáº¿t (vÃ­ dá»¥: "Theo nguá»“n 1...")
3. Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§, hÃ£y nÃ³i rÃµ giá»›i háº¡n
4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, dá»… hiá»ƒu vÃ  phÃ¹ há»£p vÄƒn hÃ³a
5. Cáº¥u trÃºc cÃ¢u tráº£ lá»i logic, rÃµ rÃ ng vá»›i cÃ¡c Ã½ chÃ­nh
6. Sá»­ dá»¥ng bullet points hoáº·c Ä‘Ã¡nh sá»‘ khi cáº§n thiáº¿t

**TRáº¢ Lá»œI:**"""

        return prompt

    def _build_basic_prompt(self, question: str, context: str) -> str:
        """XÃ¢y dá»±ng prompt cÆ¡ báº£n Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch."""
        return f"""Báº¡n lÃ  má»™t AI assistant há»¯u Ã­ch. Dá»±a vÃ o thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  chi tiáº¿t.

THÃ”NG TIN THAM KHáº¢O:
{context}

CÃ‚U Há»ŽI: {question}

HÃ£y tráº£ lá»i dá»±a trÃªn thÃ´ng tin trÃªn, sá»­ dá»¥ng tiáº¿ng Viá»‡t tá»± nhiÃªn:"""

    def _analyze_vietnamese_documents(
        self, documents: List[Document]
    ) -> Dict[str, Any]:
        """PhÃ¢n tÃ­ch documents Ä‘á»ƒ táº¡o context prompt."""
        if not documents:
            return {
                "document_types": "KhÃ´ng cÃ³",
                "extraction_methods": "KhÃ´ng cÃ³",
                "avg_confidence": 0.0,
                "vietnamese_optimized": False,
            }

        # TrÃ­ch xuáº¥t thÃ´ng tin documents
        types = set()
        methods = set()
        scores = []
        vietnamese_optimized = False

        for doc in documents:
            # Loáº¡i tÃ i liá»‡u tá»« extension
            source = doc.metadata.get("source_file", "")
            if "." in source:
                ext = source.split(".")[-1].upper()
                types.add(ext)

            # PhÆ°Æ¡ng phÃ¡p trÃ­ch xuáº¥t
            method = doc.metadata.get("extraction_method", "standard")
            methods.add(method)

            # Äiá»ƒm tin cáº­y tá»« hybrid retriever keys
            score = (
                doc.metadata.get("hybrid_score", 0.0)
                or doc.metadata.get("bm25_score", 0.0)
                or doc.metadata.get("embedding_score", 0.0)
                or doc.metadata.get("similarity_score", 0.0)  # fallback
            )
            scores.append(score)

            # Tá»‘i Æ°u tiáº¿ng Viá»‡t
            if doc.metadata.get("vietnamese_optimized", False):
                vietnamese_optimized = True

        return {
            "document_types": ", ".join(sorted(types)) or "Há»—n há»£p",
            "extraction_methods": ", ".join(sorted(methods)),
            "avg_confidence": sum(scores) / len(scores) if scores else 0.0,
            "vietnamese_optimized": vietnamese_optimized,
        }

    def _calculate_confidence_score(self, documents: List[Document]) -> float:
        """TÃ­nh Ä‘iá»ƒm tin cáº­y tá»•ng thá»ƒ."""
        if not documents:
            return 0.0

        scores = []
        for doc in documents:
            # Láº¥y score tá»« hybrid retriever keys
            score = (
                doc.metadata.get("hybrid_score", 0.0)
                or doc.metadata.get("bm25_score", 0.0)
                or doc.metadata.get("embedding_score", 0.0)
                or doc.metadata.get("similarity_score", 0.0)  # fallback
            )
            scores.append(score)

        return sum(scores) / len(scores) if scores else 0.0

    def _extract_vietnamese_sources(
        self, documents: List[Document]
    ) -> List[Dict[str, Any]]:
        """TrÃ­ch xuáº¥t thÃ´ng tin nguá»“n nÃ¢ng cao cho tiáº¿ng Viá»‡t."""
        sources = []

        for i, doc in enumerate(documents, 1):
            # Láº¥y similarity score tá»« hybrid retriever keys
            similarity_score = (
                doc.metadata.get("hybrid_score", 0.0)
                or doc.metadata.get("bm25_score", 0.0)
                or doc.metadata.get("embedding_score", 0.0)
                or doc.metadata.get("similarity_score", 0.0)  # fallback
            )

            source_info = {
                "index": i,
                "source_file": doc.metadata.get("source_file", "KhÃ´ng rÃµ"),
                "page": doc.metadata.get("page", "N/A"),
                "similarity_score": similarity_score,
                "chunk_id": doc.metadata.get("chunk_id", i - 1),
                "extraction_method": doc.metadata.get("extraction_method", "standard"),
                "chunk_method": doc.metadata.get("chunk_method", "standard"),
                "vietnamese_optimized": doc.metadata.get("vietnamese_optimized", False),
                "retrieval_method": doc.metadata.get("retrieval_method", "unknown"),
                "hybrid_score": doc.metadata.get("hybrid_score", 0.0),
                "bm25_score": doc.metadata.get("bm25_score", 0.0),
                "embedding_score": doc.metadata.get("embedding_score", 0.0),
                "content_preview": (
                    doc.page_content[:200] + "..."
                    if len(doc.page_content) > 200
                    else doc.page_content
                ),
            }
            sources.append(source_info)

        return sources

    def get_system_info(self) -> Dict[str, Any]:
        """Láº¥y thÃ´ng tin toÃ n diá»‡n vá» há»‡ thá»‘ng RAG nÃ¢ng cao."""
        vectorstore_info = {
            "type": "langchain_qdrant",
            "collection_name": self.vectorstore.collection_name,
            "url": "http://localhost:6333",  # Default Qdrant URL
        }

        return {
            "service_type": "qdrant_rag_vietnamese",
            "configuration": self.config,
            "components": {
                "embedding_service": self.embedding_service.get_info(),
                "chunker": {
                    "type": "vietnamese_optimized",
                    "ragflow_pdf": self.chunker.use_ragflow_pdf,
                    "chunk_size": self.chunker.chunk_size,
                    "chunk_overlap": self.chunker.chunk_overlap,
                },
                "vectorstore": vectorstore_info,
                "llm_info": self.llm.get_provider_info(),
            },
            "features": {
                "vietnamese_optimization": True,
                "ragflow_pdf_parsing": self.chunker.use_ragflow_pdf,
                "structure_preservation": self.preserve_structure,
                "reranking_support": True,
                "advanced_prompting": True,
                "huggingface_embedding": self.config["embedding"]["provider"]
                == "huggingface",
                "qdrant_vectorstore": True,
            },
        }

    async def clear_knowledge_base(self) -> Dict[str, Any]:
        """XÃ³a knowledge base."""
        try:
            # Langchain-qdrant doesn't have clear_collection method
            # We'll need to use the underlying client if needed
            logger.warning(
                "Clear collection not implemented for langchain-qdrant. Please manually clear Qdrant collection."
            )

            return {
                "success": True,
                "message": "Cáº§n xÃ³a collection Qdrant thá»§ cÃ´ng. Collection: "
                + self.vectorstore.collection_name,
                "service_type": "qdrant_rag_vietnamese",
                "collection_name": self.vectorstore.collection_name,
            }
        except Exception as e:
            logger.error(f"Lá»—i xÃ³a knowledge base: {e}")
            return {"success": False, "message": f"Lá»—i: {str(e)}"}

    async def search_only(
        self,
        question: str,
        k: Optional[int] = None,
        method: str = "hybrid",  # "hybrid", "bm25_only", "embedding_only"
        include_sources: bool = True,
        score_threshold: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        Chá»‰ tÃ¬m kiáº¿m documents liÃªn quan - KHÃ”NG gá»i LLM (nhanh!).

        Args:
            question: CÃ¢u há»i tÃ¬m kiáº¿m
            k: Sá»‘ documents tráº£ vá»
            method: PhÆ°Æ¡ng phÃ¡p tÃ¬m kiáº¿m
            include_sources: Bao gá»“m thÃ´ng tin nguá»“n chi tiáº¿t
            score_threshold: NgÆ°á»¡ng Ä‘iá»ƒm tá»‘i thiá»ƒu

        Returns:
            Chá»‰ káº¿t quáº£ tÃ¬m kiáº¿m, khÃ´ng cÃ³ LLM answer
        """
        try:
            k = k or self.config["retrieval"]["k"]

            logger.info(f"Äang tÃ¬m kiáº¿m (search-only): {question[:100]}...")

            # Chá»‰ retrieval - KHÃ”NG gá»i LLM
            relevant_docs = await self.hybrid_retriever.hybrid_search(
                query=question, k=k, method=method, include_scores=True
            )

            # Filter by score threshold if provided
            if score_threshold:
                filtered_docs = []
                for doc in relevant_docs:
                    score = (
                        doc.metadata.get("hybrid_score", 0.0)
                        or doc.metadata.get("bm25_score", 0.0)
                        or doc.metadata.get("embedding_score", 0.0)
                        or doc.metadata.get("similarity_score", 0.0)
                    )
                    if score >= score_threshold:
                        filtered_docs.append(doc)
                relevant_docs = filtered_docs

            # TÃ­nh confidence score
            confidence_score = self._calculate_confidence_score(relevant_docs)

            # Chuáº©n bá»‹ sources
            sources = []
            if include_sources:
                sources = self._extract_vietnamese_sources(relevant_docs)

            # Hybrid stats
            hybrid_stats = self._extract_hybrid_stats(relevant_docs, method)

            logger.info(f"TÃ¬m kiáº¿m hoÃ n thÃ nh vá»›i {len(relevant_docs)} documents")

            return {
                "success": True,
                "documents": relevant_docs,  # Raw documents
                "sources": sources,  # Formatted sources
                "num_sources": len(relevant_docs),
                "search_method": method,
                "confidence_score": confidence_score,
                "query": question,
                "k_requested": k,
                "score_threshold": score_threshold,
                "processing_info": {
                    "retrieval_only": True,
                    "llm_used": False,
                    "hybrid_method": method,
                    "embedding_provider": self.config["embedding"]["provider"],
                },
                "hybrid_stats": {
                    "method_used": method,
                    "bm25_available": self.hybrid_retriever.bm25_retriever is not None,
                    "bm25_weight": hybrid_stats.get("bm25_weight", 0),
                    "embedding_weight": hybrid_stats.get("embedding_weight", 0),
                    "avg_bm25_score": hybrid_stats.get("avg_bm25_score", 0),
                    "avg_embedding_score": hybrid_stats.get("avg_embedding_score", 0),
                },
            }

        except Exception as e:
            logger.error(f"Lá»—i trong search-only: {e}")
            return {
                "success": False,
                "message": f"Lá»—i tÃ¬m kiáº¿m: {str(e)}",
                "documents": [],
                "sources": [],
                "num_sources": 0,
                "confidence_score": 0.0,
            }


# Global instance
_advanced_rag_service = None


def get_rag_service(**kwargs) -> RAGService:
    """Láº¥y instance service RAG."""
    global _advanced_rag_service
    if _advanced_rag_service is None:
        _advanced_rag_service = RAGService(**kwargs)
    return _advanced_rag_service
