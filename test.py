import asyncio
import logging
from pathlib import Path

from core.llm import QwenLLM
from rag.parser.pdf_parser import PDFParser
from rag.chunking.chunker import Chunker
from core.embedding import EmbeddingModel
from rag.vector_store.manager import VectorStoreManager
from rag.retrieval.retriever import Retriever
import os
import dotenv

dotenv.load_dotenv()


# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s'
)
logger = logging.getLogger("test_pdf_pipeline")

# C·∫•u h√¨nh
PDF_PATH = Path("diabetes.pdf")
COLLECTION_NAME = "68b2a701f5cf267abd685844"
DOCUMENT_ID = "diabetes_main_page"
KNOWLEDGE_ID = "diabetes_2025_vn"
EMBEDDING_MODEL = None


# --- H√ÄM 1: Ingest PDF (Parse ‚Üí Chunk ‚Üí Embed ‚Üí Save) ---
async def ingest_pdf_document():
    """
    Pipeline x·ª≠ l√Ω PDF: parse ‚Üí chunk ‚Üí embed ‚Üí l∆∞u v√†o Qdrant
    """
    global EMBEDDING_MODEL

    if not PDF_PATH.exists():
        logger.error(f"‚ùå File PDF kh√¥ng t·ªìn t·∫°i: {PDF_PATH}")
        return

    logger.info(f"üìÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF: {PDF_PATH}")

    # 1. Kh·ªüi t·∫°o parser
    parser = PDFParser()

    try:
        # 2. Parse PDF
        logger.info("üîç ƒêang parse PDF...")
        parsed = await parser.parse_async(PDF_PATH)
        logger.info(f"‚úÖ Parse th√†nh c√¥ng. ƒê·ªô d√†i n·ªôi dung: {len(parsed.content)} k√Ω t·ª±")

        # 3. Chunk
        logger.info("‚úÇÔ∏è  ƒêang chunking n·ªôi dung...")
        if EMBEDDING_MODEL is None:
            EMBEDDING_MODEL = await EmbeddingModel.get_instance()
        chunker = Chunker(embedding_model=EMBEDDING_MODEL, max_tokens=512, min_tokens=50)
        chunks = await chunker.chunk_async(parsed)
        logger.info(f"‚úÖ T·∫°o ƒë∆∞·ª£c {len(chunks)} chunks")

        # 4. T·∫°o embedding
        logger.info("üß† ƒêang t·∫°o embedding...")
        texts = [chunk.content for chunk in chunks]
        embeddings = await EMBEDDING_MODEL.embed_batch(texts, max_batch_size=8)
        logger.info(f"‚úÖ T·∫°o xong {len(embeddings)} embeddings")

        # 5. T·∫°o payloads
        payloads = []
        for chunk in chunks:
            payloads.append({
                "content": chunk.content,
                "metadata": {
                    "document_id": DOCUMENT_ID,
                    "knowledge_id": KNOWLEDGE_ID,
                    "chunk_index": chunk.metadata.chunk_index,
                    "chunking_strategy": chunk.metadata.chunking_strategy
                },
                "document_is_active": True,
                "metadata": {
                    "is_active": True
                }
            })

        # 6. L∆∞u v√†o Qdrant
        vector_store = VectorStoreManager()
        await vector_store.create_collection_async(COLLECTION_NAME, size=768)
        await vector_store.insert_async(
            name=COLLECTION_NAME,
            embeddings=embeddings,
            payloads=payloads
        )

        logger.info(f"‚úÖ ƒê√£ l∆∞u {len(chunks)} chunks v√†o collection '{COLLECTION_NAME}'")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ingest: {e}", exc_info=True)
        raise


# --- H√ÄM 2: Search Retrieval ---
async def search_retrieval():
    """
    Test ph·∫ßn retrieval: t·∫°o embedding + t√¨m ki·∫øm + in k·∫øt qu·∫£.
    Kh√¥ng ph·ª• thu·ªôc v√†o parser, chunk, hay ingest.
    """
    global EMBEDDING_MODEL

    try:
        # 1. Kh·ªüi t·∫°o embedding model (singleton)
        if EMBEDDING_MODEL is None:
            EMBEDDING_MODEL = await EmbeddingModel.get_instance()
        logger.info("‚úÖ ƒê√£ kh·ªüi t·∫°o EmbeddingModel")

        # 2. Kh·ªüi t·∫°o Retriever
        retriever = Retriever(collections=[COLLECTION_NAME])
        logger.info(f"‚úÖ ƒê√£ kh·ªüi t·∫°o Retriever cho collection: {COLLECTION_NAME}")

        # 3. C√°c c√¢u h·ªèi test
        queries = [
            "B·ªánh ti·ªÉu ƒë∆∞·ªùng l√† g√¨?",
        ]

        # 4. T√¨m ki·∫øm cho t·ª´ng query
        for query in queries:
            logger.info(f"\n‚ùì C√¢u h·ªèi: {query}")

            # T·∫°o embedding
            query_vector = await EMBEDDING_MODEL.embed(query)
            logger.debug(f"üß† ƒê√£ t·∫°o embedding (size: {len(query_vector)})")

            # G·ªçi retrieve
            results = await retriever.retrieve(
                query_vector=query_vector,
                top_k=3,
                score_threshold=0.6,
                document_is_active=True,
                metadata__is_active=True
            )

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if not results:
                logger.info("  ‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p.")
            else:
                for i, r in enumerate(results):
                    content = r["payload"]["content"]
                    score = r["score"]
                    collection = r["collection"]
                    print(f"  [{i+1}] (Score: {score:.3f}) | (Col: {collection})")
                    print(f"      {content[:200]}...")

    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh search: {e}", exc_info=True)
        raise

async def test_llm():
    llm = QwenLLM(
        model=os.getenv("QWEN_MODEL"),
        base_url=os.getenv("QWEN_URL")
    )
    result = await llm.generate(
        prompt="Tui b·ªã ƒë√°i th√°o ƒë∆∞·ªùng lo·∫°i 2 th√¨ n√™n l√†m g√¨",
        temperature=0.7
    )
    print(result)


# --- H√†m main ---
async def main():
    # B∆∞·ªõc 1: Ingest
    # await ingest_pdf_document()

    # B∆∞·ªõc 2: Search
    await search_retrieval()

    # await test_llm()


if __name__ == "__main__":
    asyncio.run(main())